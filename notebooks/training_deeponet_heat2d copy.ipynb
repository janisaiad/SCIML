{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 20:29:12.226286: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-16 20:29:12.227846: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-16 20:29:12.260106: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-16 20:29:12.260919: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-16 20:29:12.956540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-03-16 20:29:14.512171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-16 20:29:14.550034: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sciml.model.deeponet import DeepONet\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_p = 40\n",
    "d_V = 80\n",
    "epochs = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(80,)),\n",
    "    tf.keras.layers.Dense(40, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(40, activation='relu'),\n",
    "])\n",
    "\n",
    "\n",
    "external_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(40, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(40, activation='relu'),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"data/test_data/example_data/heat2d/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 20:29:14,713 - sciml.model.deeponet.deeponet - INFO - Model initialized with 300 epochs, 32 batch size, 0.001 learning rate\n"
     ]
    }
   ],
   "source": [
    "model = DeepONet(regular_params={\"internal_model\": internal_model, \"external_model\": external_model}, hyper_params={\"d_p\": d_p, \"d_V\": d_V,\"device\": \"GPU\",\"n_epochs\":epochs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading mu data: 100%|██████████| 40/40 [00:00<00:00, 8196.80it/s]\n",
      "Loading x data: 100%|██████████| 40/40 [00:00<00:00, 3686.09it/s]\n",
      "Loading y data: 100%|██████████| 40/40 [00:00<00:00, 6012.26it/s]\n"
     ]
    }
   ],
   "source": [
    "mus, xs, sol = model.get_data(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 80)\n",
      "(40, 8000, 3)\n",
      "(40, 8000)\n"
     ]
    }
   ],
   "source": [
    "print(mus.shape)\n",
    "print(xs.shape)\n",
    "print(sol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading mu data: 100%|██████████| 40/40 [00:00<00:00, 6796.80it/s]\n",
      "Loading x data: 100%|██████████| 40/40 [00:00<00:00, 5347.15it/s]\n",
      "Loading y data: 100%|██████████| 40/40 [00:00<00:00, 7513.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-03-16 20:29:15.226022: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype float and shape [40,8000]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "Training progress:   0%|          | 0/300 [00:00<?, ?it/s]2025-03-16 20:29:15.258357: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype float and shape [32,8000]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2025-03-16 20:29:15.647543: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype float and shape [8,8000]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2025-03-16 20:29:15,680 - sciml.model.deeponet.deeponet - INFO - Epoch 1/300\n",
      "2025-03-16 20:29:15,681 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.308823\n",
      "2025-03-16 20:29:15,682 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.249970\n",
      "Training progress:   0%|          | 1/300 [00:00<02:08,  2.33it/s]2025-03-16 20:29:15,892 - sciml.model.deeponet.deeponet - INFO - Epoch 2/300\n",
      "2025-03-16 20:29:15,893 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.290075\n",
      "2025-03-16 20:29:15,893 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.241863\n",
      "Training progress:   1%|          | 2/300 [00:00<01:29,  3.32it/s]2025-03-16 20:29:16,094 - sciml.model.deeponet.deeponet - INFO - Epoch 3/300\n",
      "2025-03-16 20:29:16,095 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.279269\n",
      "2025-03-16 20:29:16,096 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.242699\n",
      "Training progress:   1%|          | 3/300 [00:00<01:16,  3.91it/s]2025-03-16 20:29:16,310 - sciml.model.deeponet.deeponet - INFO - Epoch 4/300\n",
      "2025-03-16 20:29:16,311 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.278004\n",
      "2025-03-16 20:29:16,312 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.239597\n",
      "Training progress:   1%|▏         | 4/300 [00:01<01:11,  4.17it/s]2025-03-16 20:29:16,522 - sciml.model.deeponet.deeponet - INFO - Epoch 5/300\n",
      "2025-03-16 20:29:16,522 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.275233\n",
      "2025-03-16 20:29:16,523 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.232910\n",
      "Training progress:   2%|▏         | 5/300 [00:01<01:07,  4.36it/s]2025-03-16 20:29:16,732 - sciml.model.deeponet.deeponet - INFO - Epoch 6/300\n",
      "2025-03-16 20:29:16,733 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.267998\n",
      "2025-03-16 20:29:16,733 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.227214\n",
      "Training progress:   2%|▏         | 6/300 [00:01<01:05,  4.48it/s]2025-03-16 20:29:16,945 - sciml.model.deeponet.deeponet - INFO - Epoch 7/300\n",
      "2025-03-16 20:29:16,946 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.260323\n",
      "2025-03-16 20:29:16,947 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.224162\n",
      "Training progress:   2%|▏         | 7/300 [00:01<01:04,  4.55it/s]2025-03-16 20:29:17,152 - sciml.model.deeponet.deeponet - INFO - Epoch 8/300\n",
      "2025-03-16 20:29:17,153 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.254491\n",
      "2025-03-16 20:29:17,154 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.222502\n",
      "Training progress:   3%|▎         | 8/300 [00:01<01:03,  4.63it/s]2025-03-16 20:29:17,362 - sciml.model.deeponet.deeponet - INFO - Epoch 9/300\n",
      "2025-03-16 20:29:17,363 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.249879\n",
      "2025-03-16 20:29:17,364 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.220646\n",
      "Training progress:   3%|▎         | 9/300 [00:02<01:02,  4.67it/s]2025-03-16 20:29:17,573 - sciml.model.deeponet.deeponet - INFO - Epoch 10/300\n",
      "2025-03-16 20:29:17,574 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.245396\n",
      "2025-03-16 20:29:17,574 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.218247\n",
      "Training progress:   3%|▎         | 10/300 [00:02<01:01,  4.70it/s]2025-03-16 20:29:17,774 - sciml.model.deeponet.deeponet - INFO - Epoch 11/300\n",
      "2025-03-16 20:29:17,774 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.241091\n",
      "2025-03-16 20:29:17,775 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.215396\n",
      "Training progress:   4%|▎         | 11/300 [00:02<01:00,  4.78it/s]2025-03-16 20:29:17,980 - sciml.model.deeponet.deeponet - INFO - Epoch 12/300\n",
      "2025-03-16 20:29:17,981 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.236867\n",
      "2025-03-16 20:29:17,982 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.212457\n",
      "Training progress:   4%|▍         | 12/300 [00:02<01:00,  4.80it/s]2025-03-16 20:29:18,183 - sciml.model.deeponet.deeponet - INFO - Epoch 13/300\n",
      "2025-03-16 20:29:18,183 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.232728\n",
      "2025-03-16 20:29:18,184 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.209570\n",
      "Training progress:   4%|▍         | 13/300 [00:02<00:59,  4.84it/s]2025-03-16 20:29:18,389 - sciml.model.deeponet.deeponet - INFO - Epoch 14/300\n",
      "2025-03-16 20:29:18,390 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.228134\n",
      "2025-03-16 20:29:18,390 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.206675\n",
      "Training progress:   5%|▍         | 14/300 [00:03<00:59,  4.84it/s]2025-03-16 20:29:18,608 - sciml.model.deeponet.deeponet - INFO - Epoch 15/300\n",
      "2025-03-16 20:29:18,608 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.222610\n",
      "2025-03-16 20:29:18,609 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.203807\n",
      "Training progress:   5%|▌         | 15/300 [00:03<00:59,  4.76it/s]2025-03-16 20:29:18,832 - sciml.model.deeponet.deeponet - INFO - Epoch 16/300\n",
      "2025-03-16 20:29:18,833 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.216474\n",
      "2025-03-16 20:29:18,834 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.200886\n",
      "Training progress:   5%|▌         | 16/300 [00:03<01:00,  4.66it/s]2025-03-16 20:29:19,046 - sciml.model.deeponet.deeponet - INFO - Epoch 17/300\n",
      "2025-03-16 20:29:19,046 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.210307\n",
      "2025-03-16 20:29:19,047 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.197641\n",
      "Training progress:   6%|▌         | 17/300 [00:03<01:00,  4.67it/s]2025-03-16 20:29:19,252 - sciml.model.deeponet.deeponet - INFO - Epoch 18/300\n",
      "2025-03-16 20:29:19,252 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.204278\n",
      "2025-03-16 20:29:19,253 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.193719\n",
      "Training progress:   6%|▌         | 18/300 [00:03<00:59,  4.72it/s]2025-03-16 20:29:19,457 - sciml.model.deeponet.deeponet - INFO - Epoch 19/300\n",
      "2025-03-16 20:29:19,458 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.197997\n",
      "2025-03-16 20:29:19,459 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.189134\n",
      "Training progress:   6%|▋         | 19/300 [00:04<00:58,  4.77it/s]2025-03-16 20:29:19,665 - sciml.model.deeponet.deeponet - INFO - Epoch 20/300\n",
      "2025-03-16 20:29:19,666 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.191268\n",
      "2025-03-16 20:29:19,667 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.184522\n",
      "Training progress:   7%|▋         | 20/300 [00:04<00:58,  4.78it/s]2025-03-16 20:29:19,871 - sciml.model.deeponet.deeponet - INFO - Epoch 21/300\n",
      "2025-03-16 20:29:19,872 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.184662\n",
      "2025-03-16 20:29:19,872 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.180419\n",
      "Training progress:   7%|▋         | 21/300 [00:04<00:58,  4.81it/s]2025-03-16 20:29:20,078 - sciml.model.deeponet.deeponet - INFO - Epoch 22/300\n",
      "2025-03-16 20:29:20,079 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.178306\n",
      "2025-03-16 20:29:20,088 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.176878\n",
      "Training progress:   7%|▋         | 22/300 [00:04<00:58,  4.75it/s]2025-03-16 20:29:20,291 - sciml.model.deeponet.deeponet - INFO - Epoch 23/300\n",
      "2025-03-16 20:29:20,292 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.171618\n",
      "2025-03-16 20:29:20,292 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.174122\n",
      "Training progress:   8%|▊         | 23/300 [00:05<00:57,  4.79it/s]2025-03-16 20:29:20,493 - sciml.model.deeponet.deeponet - INFO - Epoch 24/300\n",
      "2025-03-16 20:29:20,494 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.165243\n",
      "2025-03-16 20:29:20,495 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.171973\n",
      "Training progress:   8%|▊         | 24/300 [00:05<00:57,  4.84it/s]2025-03-16 20:29:20,703 - sciml.model.deeponet.deeponet - INFO - Epoch 25/300\n",
      "2025-03-16 20:29:20,704 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.159804\n",
      "2025-03-16 20:29:20,705 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.169190\n",
      "Training progress:   8%|▊         | 25/300 [00:05<00:57,  4.82it/s]2025-03-16 20:29:20,909 - sciml.model.deeponet.deeponet - INFO - Epoch 26/300\n",
      "2025-03-16 20:29:20,910 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.154434\n",
      "2025-03-16 20:29:20,911 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.165751\n",
      "Training progress:   9%|▊         | 26/300 [00:05<00:56,  4.83it/s]2025-03-16 20:29:21,112 - sciml.model.deeponet.deeponet - INFO - Epoch 27/300\n",
      "2025-03-16 20:29:21,113 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.149622\n",
      "2025-03-16 20:29:21,114 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.162891\n",
      "Training progress:   9%|▉         | 27/300 [00:05<00:56,  4.85it/s]2025-03-16 20:29:21,321 - sciml.model.deeponet.deeponet - INFO - Epoch 28/300\n",
      "2025-03-16 20:29:21,322 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.146029\n",
      "2025-03-16 20:29:21,322 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.161347\n",
      "Training progress:   9%|▉         | 28/300 [00:06<00:56,  4.84it/s]2025-03-16 20:29:21,525 - sciml.model.deeponet.deeponet - INFO - Epoch 29/300\n",
      "2025-03-16 20:29:21,526 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.142756\n",
      "2025-03-16 20:29:21,526 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.160821\n",
      "Training progress:  10%|▉         | 29/300 [00:06<00:55,  4.86it/s]2025-03-16 20:29:21,730 - sciml.model.deeponet.deeponet - INFO - Epoch 30/300\n",
      "2025-03-16 20:29:21,730 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.140455\n",
      "2025-03-16 20:29:21,731 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.159447\n",
      "Training progress:  10%|█         | 30/300 [00:06<00:55,  4.87it/s]2025-03-16 20:29:21,938 - sciml.model.deeponet.deeponet - INFO - Epoch 31/300\n",
      "2025-03-16 20:29:21,939 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.138751\n",
      "2025-03-16 20:29:21,939 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.156765\n",
      "Training progress:  10%|█         | 31/300 [00:06<00:55,  4.84it/s]2025-03-16 20:29:22,145 - sciml.model.deeponet.deeponet - INFO - Epoch 32/300\n",
      "2025-03-16 20:29:22,146 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.137441\n",
      "2025-03-16 20:29:22,147 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.154959\n",
      "Training progress:  11%|█         | 32/300 [00:06<00:55,  4.84it/s]2025-03-16 20:29:22,355 - sciml.model.deeponet.deeponet - INFO - Epoch 33/300\n",
      "2025-03-16 20:29:22,355 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.136954\n",
      "2025-03-16 20:29:22,356 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.154976\n",
      "Training progress:  11%|█         | 33/300 [00:07<00:55,  4.82it/s]2025-03-16 20:29:22,560 - sciml.model.deeponet.deeponet - INFO - Epoch 34/300\n",
      "2025-03-16 20:29:22,561 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.136127\n",
      "2025-03-16 20:29:22,561 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.155245\n",
      "Training progress:  11%|█▏        | 34/300 [00:07<00:55,  4.83it/s]2025-03-16 20:29:22,770 - sciml.model.deeponet.deeponet - INFO - Epoch 35/300\n",
      "2025-03-16 20:29:22,771 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.135588\n",
      "2025-03-16 20:29:22,771 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.153422\n",
      "Training progress:  12%|█▏        | 35/300 [00:07<00:55,  4.81it/s]2025-03-16 20:29:22,972 - sciml.model.deeponet.deeponet - INFO - Epoch 36/300\n",
      "2025-03-16 20:29:22,972 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.134475\n",
      "2025-03-16 20:29:22,973 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.150982\n",
      "Training progress:  12%|█▏        | 36/300 [00:07<00:54,  4.85it/s]2025-03-16 20:29:23,173 - sciml.model.deeponet.deeponet - INFO - Epoch 37/300\n",
      "2025-03-16 20:29:23,174 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.133359\n",
      "2025-03-16 20:29:23,175 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.149995\n",
      "Training progress:  12%|█▏        | 37/300 [00:07<00:53,  4.89it/s]2025-03-16 20:29:23,376 - sciml.model.deeponet.deeponet - INFO - Epoch 38/300\n",
      "2025-03-16 20:29:23,377 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.131890\n",
      "2025-03-16 20:29:23,377 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.149871\n",
      "Training progress:  13%|█▎        | 38/300 [00:08<00:53,  4.90it/s]2025-03-16 20:29:23,582 - sciml.model.deeponet.deeponet - INFO - Epoch 39/300\n",
      "2025-03-16 20:29:23,583 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.130467\n",
      "2025-03-16 20:29:23,583 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.148390\n",
      "Training progress:  13%|█▎        | 39/300 [00:08<00:53,  4.89it/s]2025-03-16 20:29:23,784 - sciml.model.deeponet.deeponet - INFO - Epoch 40/300\n",
      "2025-03-16 20:29:23,784 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.128909\n",
      "2025-03-16 20:29:23,785 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.146536\n",
      "Training progress:  13%|█▎        | 40/300 [00:08<00:52,  4.91it/s]2025-03-16 20:29:23,987 - sciml.model.deeponet.deeponet - INFO - Epoch 41/300\n",
      "2025-03-16 20:29:23,988 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.127561\n",
      "2025-03-16 20:29:23,988 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.145671\n",
      "Training progress:  14%|█▎        | 41/300 [00:08<00:52,  4.91it/s]2025-03-16 20:29:24,188 - sciml.model.deeponet.deeponet - INFO - Epoch 42/300\n",
      "2025-03-16 20:29:24,188 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.126225\n",
      "2025-03-16 20:29:24,189 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.145186\n",
      "Training progress:  14%|█▍        | 42/300 [00:08<00:52,  4.93it/s]2025-03-16 20:29:24,392 - sciml.model.deeponet.deeponet - INFO - Epoch 43/300\n",
      "2025-03-16 20:29:24,393 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.125098\n",
      "2025-03-16 20:29:24,394 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.143832\n",
      "Training progress:  14%|█▍        | 43/300 [00:09<00:52,  4.92it/s]2025-03-16 20:29:24,601 - sciml.model.deeponet.deeponet - INFO - Epoch 44/300\n",
      "2025-03-16 20:29:24,602 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.124040\n",
      "2025-03-16 20:29:24,602 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.142489\n",
      "Training progress:  15%|█▍        | 44/300 [00:09<00:52,  4.88it/s]2025-03-16 20:29:24,814 - sciml.model.deeponet.deeponet - INFO - Epoch 45/300\n",
      "2025-03-16 20:29:24,815 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.123236\n",
      "2025-03-16 20:29:24,816 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.141769\n",
      "Training progress:  15%|█▌        | 45/300 [00:09<00:52,  4.82it/s]2025-03-16 20:29:25,035 - sciml.model.deeponet.deeponet - INFO - Epoch 46/300\n",
      "2025-03-16 20:29:25,036 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.122313\n",
      "2025-03-16 20:29:25,036 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.141197\n",
      "Training progress:  15%|█▌        | 46/300 [00:09<00:53,  4.73it/s]2025-03-16 20:29:25,244 - sciml.model.deeponet.deeponet - INFO - Epoch 47/300\n",
      "2025-03-16 20:29:25,245 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.121472\n",
      "2025-03-16 20:29:25,246 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.140211\n",
      "Training progress:  16%|█▌        | 47/300 [00:09<00:53,  4.74it/s]2025-03-16 20:29:25,453 - sciml.model.deeponet.deeponet - INFO - Epoch 48/300\n",
      "2025-03-16 20:29:25,454 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.120501\n",
      "2025-03-16 20:29:25,454 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.139210\n",
      "Training progress:  16%|█▌        | 48/300 [00:10<00:52,  4.76it/s]2025-03-16 20:29:25,655 - sciml.model.deeponet.deeponet - INFO - Epoch 49/300\n",
      "2025-03-16 20:29:25,656 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.119530\n",
      "2025-03-16 20:29:25,656 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.138348\n",
      "Training progress:  16%|█▋        | 49/300 [00:10<00:52,  4.82it/s]2025-03-16 20:29:25,862 - sciml.model.deeponet.deeponet - INFO - Epoch 50/300\n",
      "2025-03-16 20:29:25,863 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.118463\n",
      "2025-03-16 20:29:25,863 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.137552\n",
      "Training progress:  17%|█▋        | 50/300 [00:10<00:51,  4.82it/s]2025-03-16 20:29:26,069 - sciml.model.deeponet.deeponet - INFO - Epoch 51/300\n",
      "2025-03-16 20:29:26,070 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.117516\n",
      "2025-03-16 20:29:26,070 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.136572\n",
      "Training progress:  17%|█▋        | 51/300 [00:10<00:51,  4.82it/s]2025-03-16 20:29:26,280 - sciml.model.deeponet.deeponet - INFO - Epoch 52/300\n",
      "2025-03-16 20:29:26,281 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.116570\n",
      "2025-03-16 20:29:26,281 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.135523\n",
      "Training progress:  17%|█▋        | 52/300 [00:11<00:51,  4.80it/s]2025-03-16 20:29:26,488 - sciml.model.deeponet.deeponet - INFO - Epoch 53/300\n",
      "2025-03-16 20:29:26,488 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.115704\n",
      "2025-03-16 20:29:26,489 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.134499\n",
      "Training progress:  18%|█▊        | 53/300 [00:11<00:51,  4.80it/s]2025-03-16 20:29:26,693 - sciml.model.deeponet.deeponet - INFO - Epoch 54/300\n",
      "2025-03-16 20:29:26,694 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.114780\n",
      "2025-03-16 20:29:26,694 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.133555\n",
      "Training progress:  18%|█▊        | 54/300 [00:11<00:51,  4.82it/s]2025-03-16 20:29:26,901 - sciml.model.deeponet.deeponet - INFO - Epoch 55/300\n",
      "2025-03-16 20:29:26,902 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.113887\n",
      "2025-03-16 20:29:26,903 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.132419\n",
      "Training progress:  18%|█▊        | 55/300 [00:11<00:50,  4.82it/s]2025-03-16 20:29:27,107 - sciml.model.deeponet.deeponet - INFO - Epoch 56/300\n",
      "2025-03-16 20:29:27,108 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.112901\n",
      "2025-03-16 20:29:27,109 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.131418\n",
      "Training progress:  19%|█▊        | 56/300 [00:11<00:50,  4.83it/s]2025-03-16 20:29:27,311 - sciml.model.deeponet.deeponet - INFO - Epoch 57/300\n",
      "2025-03-16 20:29:27,312 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.111952\n",
      "2025-03-16 20:29:27,313 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.130687\n",
      "Training progress:  19%|█▉        | 57/300 [00:12<00:50,  4.85it/s]2025-03-16 20:29:27,518 - sciml.model.deeponet.deeponet - INFO - Epoch 58/300\n",
      "2025-03-16 20:29:27,519 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.111047\n",
      "2025-03-16 20:29:27,520 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.129796\n",
      "Training progress:  19%|█▉        | 58/300 [00:12<00:49,  4.84it/s]2025-03-16 20:29:27,726 - sciml.model.deeponet.deeponet - INFO - Epoch 59/300\n",
      "2025-03-16 20:29:27,727 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.110239\n",
      "2025-03-16 20:29:27,727 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.128930\n",
      "Training progress:  20%|█▉        | 59/300 [00:12<00:49,  4.84it/s]2025-03-16 20:29:27,934 - sciml.model.deeponet.deeponet - INFO - Epoch 60/300\n",
      "2025-03-16 20:29:27,934 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.109518\n",
      "2025-03-16 20:29:27,935 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.128407\n",
      "Training progress:  20%|██        | 60/300 [00:12<00:49,  4.83it/s]2025-03-16 20:29:28,140 - sciml.model.deeponet.deeponet - INFO - Epoch 61/300\n",
      "2025-03-16 20:29:28,141 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.108786\n",
      "2025-03-16 20:29:28,141 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.127597\n",
      "Training progress:  20%|██        | 61/300 [00:12<00:49,  4.83it/s]2025-03-16 20:29:28,345 - sciml.model.deeponet.deeponet - INFO - Epoch 62/300\n",
      "2025-03-16 20:29:28,346 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.108012\n",
      "2025-03-16 20:29:28,346 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.126738\n",
      "Training progress:  21%|██        | 62/300 [00:13<00:49,  4.85it/s]2025-03-16 20:29:28,553 - sciml.model.deeponet.deeponet - INFO - Epoch 63/300\n",
      "2025-03-16 20:29:28,554 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.107208\n",
      "2025-03-16 20:29:28,555 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.126140\n",
      "Training progress:  21%|██        | 63/300 [00:13<00:49,  4.83it/s]2025-03-16 20:29:28,781 - sciml.model.deeponet.deeponet - INFO - Epoch 64/300\n",
      "2025-03-16 20:29:28,782 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.106419\n",
      "2025-03-16 20:29:28,782 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.125004\n",
      "Training progress:  21%|██▏       | 64/300 [00:13<00:50,  4.69it/s]2025-03-16 20:29:28,993 - sciml.model.deeponet.deeponet - INFO - Epoch 65/300\n",
      "2025-03-16 20:29:28,993 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.105638\n",
      "2025-03-16 20:29:28,994 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.124451\n",
      "Training progress:  22%|██▏       | 65/300 [00:13<00:49,  4.70it/s]2025-03-16 20:29:29,208 - sciml.model.deeponet.deeponet - INFO - Epoch 66/300\n",
      "2025-03-16 20:29:29,208 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.104898\n",
      "2025-03-16 20:29:29,209 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.123637\n",
      "Training progress:  22%|██▏       | 66/300 [00:13<00:49,  4.68it/s]2025-03-16 20:29:29,425 - sciml.model.deeponet.deeponet - INFO - Epoch 67/300\n",
      "2025-03-16 20:29:29,426 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.104201\n",
      "2025-03-16 20:29:29,426 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.122606\n",
      "Training progress:  22%|██▏       | 67/300 [00:14<00:49,  4.66it/s]2025-03-16 20:29:29,650 - sciml.model.deeponet.deeponet - INFO - Epoch 68/300\n",
      "2025-03-16 20:29:29,651 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.103490\n",
      "2025-03-16 20:29:29,651 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.122167\n",
      "Training progress:  23%|██▎       | 68/300 [00:14<00:50,  4.59it/s]2025-03-16 20:29:29,877 - sciml.model.deeponet.deeponet - INFO - Epoch 69/300\n",
      "2025-03-16 20:29:29,878 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.102761\n",
      "2025-03-16 20:29:29,887 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.120740\n",
      "Training progress:  23%|██▎       | 69/300 [00:14<00:51,  4.48it/s]2025-03-16 20:29:30,094 - sciml.model.deeponet.deeponet - INFO - Epoch 70/300\n",
      "2025-03-16 20:29:30,095 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.102061\n",
      "2025-03-16 20:29:30,095 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.120296\n",
      "Training progress:  23%|██▎       | 70/300 [00:14<00:50,  4.57it/s]2025-03-16 20:29:30,302 - sciml.model.deeponet.deeponet - INFO - Epoch 71/300\n",
      "2025-03-16 20:29:30,303 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.101411\n",
      "2025-03-16 20:29:30,304 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.118397\n",
      "Training progress:  24%|██▎       | 71/300 [00:15<00:49,  4.64it/s]2025-03-16 20:29:30,509 - sciml.model.deeponet.deeponet - INFO - Epoch 72/300\n",
      "2025-03-16 20:29:30,510 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.100880\n",
      "2025-03-16 20:29:30,511 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.118988\n",
      "Training progress:  24%|██▍       | 72/300 [00:15<00:48,  4.69it/s]2025-03-16 20:29:30,713 - sciml.model.deeponet.deeponet - INFO - Epoch 73/300\n",
      "2025-03-16 20:29:30,714 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.100466\n",
      "2025-03-16 20:29:30,715 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.116422\n",
      "Training progress:  24%|██▍       | 73/300 [00:15<00:47,  4.76it/s]2025-03-16 20:29:30,917 - sciml.model.deeponet.deeponet - INFO - Epoch 74/300\n",
      "2025-03-16 20:29:30,918 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.100094\n",
      "2025-03-16 20:29:30,919 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.118311\n",
      "Training progress:  25%|██▍       | 74/300 [00:15<00:47,  4.80it/s]2025-03-16 20:29:31,119 - sciml.model.deeponet.deeponet - INFO - Epoch 75/300\n",
      "2025-03-16 20:29:31,120 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.099895\n",
      "2025-03-16 20:29:31,121 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.115032\n",
      "Training progress:  25%|██▌       | 75/300 [00:15<00:46,  4.84it/s]2025-03-16 20:29:31,328 - sciml.model.deeponet.deeponet - INFO - Epoch 76/300\n",
      "2025-03-16 20:29:31,329 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.098949\n",
      "2025-03-16 20:29:31,330 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.115175\n",
      "Training progress:  25%|██▌       | 76/300 [00:16<00:46,  4.83it/s]2025-03-16 20:29:31,538 - sciml.model.deeponet.deeponet - INFO - Epoch 77/300\n",
      "2025-03-16 20:29:31,538 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.097985\n",
      "2025-03-16 20:29:31,539 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.114946\n",
      "Training progress:  26%|██▌       | 77/300 [00:16<00:46,  4.81it/s]2025-03-16 20:29:31,743 - sciml.model.deeponet.deeponet - INFO - Epoch 78/300\n",
      "2025-03-16 20:29:31,744 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.097638\n",
      "2025-03-16 20:29:31,745 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.113116\n",
      "Training progress:  26%|██▌       | 78/300 [00:16<00:46,  4.82it/s]2025-03-16 20:29:31,946 - sciml.model.deeponet.deeponet - INFO - Epoch 79/300\n",
      "2025-03-16 20:29:31,947 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.097492\n",
      "2025-03-16 20:29:31,947 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.114464\n",
      "Training progress:  26%|██▋       | 79/300 [00:16<00:45,  4.86it/s]2025-03-16 20:29:32,154 - sciml.model.deeponet.deeponet - INFO - Epoch 80/300\n",
      "2025-03-16 20:29:32,154 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.097105\n",
      "2025-03-16 20:29:32,155 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.112273\n",
      "Training progress:  27%|██▋       | 80/300 [00:16<00:45,  4.84it/s]2025-03-16 20:29:32,358 - sciml.model.deeponet.deeponet - INFO - Epoch 81/300\n",
      "2025-03-16 20:29:32,359 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.096315\n",
      "2025-03-16 20:29:32,360 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.112049\n",
      "Training progress:  27%|██▋       | 81/300 [00:17<00:45,  4.86it/s]2025-03-16 20:29:32,564 - sciml.model.deeponet.deeponet - INFO - Epoch 82/300\n",
      "2025-03-16 20:29:32,565 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.095751\n",
      "2025-03-16 20:29:32,566 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.112343\n",
      "Training progress:  27%|██▋       | 82/300 [00:17<00:44,  4.85it/s]2025-03-16 20:29:32,771 - sciml.model.deeponet.deeponet - INFO - Epoch 83/300\n",
      "2025-03-16 20:29:32,772 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.095599\n",
      "2025-03-16 20:29:32,773 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.110272\n",
      "Training progress:  28%|██▊       | 83/300 [00:17<00:44,  4.85it/s]2025-03-16 20:29:32,977 - sciml.model.deeponet.deeponet - INFO - Epoch 84/300\n",
      "2025-03-16 20:29:32,978 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.095409\n",
      "2025-03-16 20:29:32,979 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.111244\n",
      "Training progress:  28%|██▊       | 84/300 [00:17<00:44,  4.85it/s]2025-03-16 20:29:33,183 - sciml.model.deeponet.deeponet - INFO - Epoch 85/300\n",
      "2025-03-16 20:29:33,184 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.095023\n",
      "2025-03-16 20:29:33,184 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.109285\n",
      "Training progress:  28%|██▊       | 85/300 [00:17<00:44,  4.86it/s]2025-03-16 20:29:33,387 - sciml.model.deeponet.deeponet - INFO - Epoch 86/300\n",
      "2025-03-16 20:29:33,388 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.094292\n",
      "2025-03-16 20:29:33,389 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.108827\n",
      "Training progress:  29%|██▊       | 86/300 [00:18<00:43,  4.86it/s]2025-03-16 20:29:33,594 - sciml.model.deeponet.deeponet - INFO - Epoch 87/300\n",
      "2025-03-16 20:29:33,595 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.093938\n",
      "2025-03-16 20:29:33,596 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.109662\n",
      "Training progress:  29%|██▉       | 87/300 [00:18<00:43,  4.86it/s]2025-03-16 20:29:33,799 - sciml.model.deeponet.deeponet - INFO - Epoch 88/300\n",
      "2025-03-16 20:29:33,800 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.093881\n",
      "2025-03-16 20:29:33,801 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.107919\n",
      "Training progress:  29%|██▉       | 88/300 [00:18<00:43,  4.86it/s]2025-03-16 20:29:34,002 - sciml.model.deeponet.deeponet - INFO - Epoch 89/300\n",
      "2025-03-16 20:29:34,003 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.093520\n",
      "2025-03-16 20:29:34,004 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.108442\n",
      "Training progress:  30%|██▉       | 89/300 [00:18<00:43,  4.88it/s]2025-03-16 20:29:34,209 - sciml.model.deeponet.deeponet - INFO - Epoch 90/300\n",
      "2025-03-16 20:29:34,210 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.092999\n",
      "2025-03-16 20:29:34,210 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.107494\n",
      "Training progress:  30%|███       | 90/300 [00:18<00:43,  4.87it/s]2025-03-16 20:29:34,414 - sciml.model.deeponet.deeponet - INFO - Epoch 91/300\n",
      "2025-03-16 20:29:34,415 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.092537\n",
      "2025-03-16 20:29:34,415 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.106701\n",
      "Training progress:  30%|███       | 91/300 [00:19<00:42,  4.87it/s]2025-03-16 20:29:34,622 - sciml.model.deeponet.deeponet - INFO - Epoch 92/300\n",
      "2025-03-16 20:29:34,623 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.092339\n",
      "2025-03-16 20:29:34,623 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.107347\n",
      "Training progress:  31%|███       | 92/300 [00:19<00:42,  4.85it/s]2025-03-16 20:29:34,826 - sciml.model.deeponet.deeponet - INFO - Epoch 93/300\n",
      "2025-03-16 20:29:34,826 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.092245\n",
      "2025-03-16 20:29:34,827 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.105946\n",
      "Training progress:  31%|███       | 93/300 [00:19<00:42,  4.87it/s]2025-03-16 20:29:35,030 - sciml.model.deeponet.deeponet - INFO - Epoch 94/300\n",
      "2025-03-16 20:29:35,031 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.091949\n",
      "2025-03-16 20:29:35,032 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.106478\n",
      "Training progress:  31%|███▏      | 94/300 [00:19<00:42,  4.87it/s]2025-03-16 20:29:35,231 - sciml.model.deeponet.deeponet - INFO - Epoch 95/300\n",
      "2025-03-16 20:29:35,232 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.091510\n",
      "2025-03-16 20:29:35,233 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.105381\n",
      "Training progress:  32%|███▏      | 95/300 [00:19<00:41,  4.91it/s]2025-03-16 20:29:35,433 - sciml.model.deeponet.deeponet - INFO - Epoch 96/300\n",
      "2025-03-16 20:29:35,434 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.091024\n",
      "2025-03-16 20:29:35,435 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.104824\n",
      "Training progress:  32%|███▏      | 96/300 [00:20<00:41,  4.92it/s]2025-03-16 20:29:35,641 - sciml.model.deeponet.deeponet - INFO - Epoch 97/300\n",
      "2025-03-16 20:29:35,642 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.090796\n",
      "2025-03-16 20:29:35,642 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.105328\n",
      "Training progress:  32%|███▏      | 97/300 [00:20<00:41,  4.89it/s]2025-03-16 20:29:35,844 - sciml.model.deeponet.deeponet - INFO - Epoch 98/300\n",
      "2025-03-16 20:29:35,845 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.090707\n",
      "2025-03-16 20:29:35,846 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.104210\n",
      "Training progress:  33%|███▎      | 98/300 [00:20<00:41,  4.90it/s]2025-03-16 20:29:36,050 - sciml.model.deeponet.deeponet - INFO - Epoch 99/300\n",
      "2025-03-16 20:29:36,051 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.090560\n",
      "2025-03-16 20:29:36,052 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.104975\n",
      "Training progress:  33%|███▎      | 99/300 [00:20<00:41,  4.88it/s]2025-03-16 20:29:36,259 - sciml.model.deeponet.deeponet - INFO - Epoch 100/300\n",
      "2025-03-16 20:29:36,260 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.090351\n",
      "2025-03-16 20:29:36,260 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.103386\n",
      "Training progress:  33%|███▎      | 100/300 [00:21<00:41,  4.86it/s]2025-03-16 20:29:36,463 - sciml.model.deeponet.deeponet - INFO - Epoch 101/300\n",
      "2025-03-16 20:29:36,464 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.089882\n",
      "2025-03-16 20:29:36,465 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.103306\n",
      "Training progress:  34%|███▎      | 101/300 [00:21<00:40,  4.87it/s]2025-03-16 20:29:36,666 - sciml.model.deeponet.deeponet - INFO - Epoch 102/300\n",
      "2025-03-16 20:29:36,666 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.089464\n",
      "2025-03-16 20:29:36,667 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.102806\n",
      "Training progress:  34%|███▍      | 102/300 [00:21<00:40,  4.89it/s]2025-03-16 20:29:36,869 - sciml.model.deeponet.deeponet - INFO - Epoch 103/300\n",
      "2025-03-16 20:29:36,870 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.089172\n",
      "2025-03-16 20:29:36,871 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.102303\n",
      "Training progress:  34%|███▍      | 103/300 [00:21<00:40,  4.89it/s]2025-03-16 20:29:37,074 - sciml.model.deeponet.deeponet - INFO - Epoch 104/300\n",
      "2025-03-16 20:29:37,075 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.089003\n",
      "2025-03-16 20:29:37,075 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.102619\n",
      "Training progress:  35%|███▍      | 104/300 [00:21<00:40,  4.90it/s]2025-03-16 20:29:37,280 - sciml.model.deeponet.deeponet - INFO - Epoch 105/300\n",
      "2025-03-16 20:29:37,281 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.088901\n",
      "2025-03-16 20:29:37,281 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.101565\n",
      "Training progress:  35%|███▌      | 105/300 [00:22<00:39,  4.88it/s]2025-03-16 20:29:37,483 - sciml.model.deeponet.deeponet - INFO - Epoch 106/300\n",
      "2025-03-16 20:29:37,484 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.088761\n",
      "2025-03-16 20:29:37,485 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.102212\n",
      "Training progress:  35%|███▌      | 106/300 [00:22<00:39,  4.89it/s]2025-03-16 20:29:37,687 - sciml.model.deeponet.deeponet - INFO - Epoch 107/300\n",
      "2025-03-16 20:29:37,687 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.088693\n",
      "2025-03-16 20:29:37,688 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.101039\n",
      "Training progress:  36%|███▌      | 107/300 [00:22<00:39,  4.90it/s]2025-03-16 20:29:37,890 - sciml.model.deeponet.deeponet - INFO - Epoch 108/300\n",
      "2025-03-16 20:29:37,891 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.088496\n",
      "2025-03-16 20:29:37,891 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.101685\n",
      "Training progress:  36%|███▌      | 108/300 [00:22<00:39,  4.91it/s]2025-03-16 20:29:38,092 - sciml.model.deeponet.deeponet - INFO - Epoch 109/300\n",
      "2025-03-16 20:29:38,093 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.088242\n",
      "2025-03-16 20:29:38,094 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.100534\n",
      "Training progress:  36%|███▋      | 109/300 [00:22<00:38,  4.91it/s]2025-03-16 20:29:38,295 - sciml.model.deeponet.deeponet - INFO - Epoch 110/300\n",
      "2025-03-16 20:29:38,296 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.087808\n",
      "2025-03-16 20:29:38,297 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.100724\n",
      "Training progress:  37%|███▋      | 110/300 [00:23<00:38,  4.92it/s]2025-03-16 20:29:38,535 - sciml.model.deeponet.deeponet - INFO - Epoch 111/300\n",
      "2025-03-16 20:29:38,536 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.087446\n",
      "2025-03-16 20:29:38,536 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.100011\n",
      "Training progress:  37%|███▋      | 111/300 [00:23<00:40,  4.67it/s]2025-03-16 20:29:38,759 - sciml.model.deeponet.deeponet - INFO - Epoch 112/300\n",
      "2025-03-16 20:29:38,760 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.087129\n",
      "2025-03-16 20:29:38,761 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.099545\n",
      "Training progress:  37%|███▋      | 112/300 [00:23<00:40,  4.61it/s]2025-03-16 20:29:38,959 - sciml.model.deeponet.deeponet - INFO - Epoch 113/300\n",
      "2025-03-16 20:29:38,960 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.086913\n",
      "2025-03-16 20:29:38,961 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.099596\n",
      "Training progress:  38%|███▊      | 113/300 [00:23<00:39,  4.71it/s]2025-03-16 20:29:36,235 - sciml.model.deeponet.deeponet - INFO - Epoch 114/300\n",
      "2025-03-16 20:29:36,235 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.086764\n",
      "2025-03-16 20:29:36,236 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.098776\n",
      "2025-03-16 20:29:36,446 - sciml.model.deeponet.deeponet - INFO - Epoch 115/300\n",
      "2025-03-16 20:29:36,447 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.086653\n",
      "2025-03-16 20:29:36,448 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.099926\n",
      "2025-03-16 20:29:36,646 - sciml.model.deeponet.deeponet - INFO - Epoch 116/300\n",
      "2025-03-16 20:29:36,646 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.086738\n",
      "2025-03-16 20:29:36,647 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.098005\n",
      "2025-03-16 20:29:36,848 - sciml.model.deeponet.deeponet - INFO - Epoch 117/300\n",
      "2025-03-16 20:29:36,849 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.086862\n",
      "2025-03-16 20:29:36,850 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.100764\n",
      "2025-03-16 20:29:37,049 - sciml.model.deeponet.deeponet - INFO - Epoch 118/300\n",
      "2025-03-16 20:29:37,050 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.087441\n",
      "2025-03-16 20:29:37,051 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.097609\n",
      "2025-03-16 20:29:37,248 - sciml.model.deeponet.deeponet - INFO - Epoch 119/300\n",
      "2025-03-16 20:29:37,249 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.087138\n",
      "2025-03-16 20:29:37,250 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.099833\n",
      "2025-03-16 20:29:37,451 - sciml.model.deeponet.deeponet - INFO - Epoch 120/300\n",
      "2025-03-16 20:29:37,452 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.086659\n",
      "2025-03-16 20:29:37,452 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.097188\n",
      "2025-03-16 20:29:37,656 - sciml.model.deeponet.deeponet - INFO - Epoch 121/300\n",
      "2025-03-16 20:29:37,657 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.085465\n",
      "2025-03-16 20:29:37,657 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.097226\n",
      "2025-03-16 20:29:37,859 - sciml.model.deeponet.deeponet - INFO - Epoch 122/300\n",
      "2025-03-16 20:29:37,860 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.085337\n",
      "2025-03-16 20:29:37,861 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.099541\n",
      "2025-03-16 20:29:38,062 - sciml.model.deeponet.deeponet - INFO - Epoch 123/300\n",
      "2025-03-16 20:29:38,062 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.086148\n",
      "2025-03-16 20:29:38,063 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.096744\n",
      "2025-03-16 20:29:38,258 - sciml.model.deeponet.deeponet - INFO - Epoch 124/300\n",
      "2025-03-16 20:29:38,259 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.086154\n",
      "2025-03-16 20:29:38,260 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.099208\n",
      "2025-03-16 20:29:38,462 - sciml.model.deeponet.deeponet - INFO - Epoch 125/300\n",
      "2025-03-16 20:29:38,463 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.085578\n",
      "2025-03-16 20:29:38,463 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.096798\n",
      "2025-03-16 20:29:38,667 - sciml.model.deeponet.deeponet - INFO - Epoch 126/300\n",
      "2025-03-16 20:29:38,668 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.084607\n",
      "2025-03-16 20:29:38,669 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.095838\n",
      "2025-03-16 20:29:38,871 - sciml.model.deeponet.deeponet - INFO - Epoch 127/300\n",
      "2025-03-16 20:29:38,872 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.084860\n",
      "2025-03-16 20:29:38,873 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.099270\n",
      "2025-03-16 20:29:39,073 - sciml.model.deeponet.deeponet - INFO - Epoch 128/300\n",
      "2025-03-16 20:29:39,073 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.085560\n",
      "2025-03-16 20:29:39,074 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.096294\n",
      "Training progress:  43%|████▎     | 128/300 [00:23<00:06, 28.53it/s]2025-03-16 20:29:39,275 - sciml.model.deeponet.deeponet - INFO - Epoch 129/300\n",
      "2025-03-16 20:29:39,275 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.084944\n",
      "2025-03-16 20:29:39,276 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.096767\n",
      "2025-03-16 20:29:39,474 - sciml.model.deeponet.deeponet - INFO - Epoch 130/300\n",
      "2025-03-16 20:29:39,475 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.084176\n",
      "2025-03-16 20:29:39,476 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.096691\n",
      "2025-03-16 20:29:39,679 - sciml.model.deeponet.deeponet - INFO - Epoch 131/300\n",
      "2025-03-16 20:29:39,680 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.083954\n",
      "2025-03-16 20:29:39,680 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.095318\n",
      "2025-03-16 20:29:39,881 - sciml.model.deeponet.deeponet - INFO - Epoch 132/300\n",
      "2025-03-16 20:29:39,882 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.084298\n",
      "2025-03-16 20:29:39,882 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.097177\n",
      "2025-03-16 20:29:40,085 - sciml.model.deeponet.deeponet - INFO - Epoch 133/300\n",
      "2025-03-16 20:29:40,085 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.084394\n",
      "2025-03-16 20:29:40,086 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.095371\n",
      "Training progress:  44%|████▍     | 133/300 [00:24<00:14, 11.92it/s]2025-03-16 20:29:40,287 - sciml.model.deeponet.deeponet - INFO - Epoch 134/300\n",
      "2025-03-16 20:29:40,288 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.083677\n",
      "2025-03-16 20:29:40,289 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.095699\n",
      "2025-03-16 20:29:40,487 - sciml.model.deeponet.deeponet - INFO - Epoch 135/300\n",
      "2025-03-16 20:29:40,488 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.083399\n",
      "2025-03-16 20:29:40,489 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.095936\n",
      "2025-03-16 20:29:40,690 - sciml.model.deeponet.deeponet - INFO - Epoch 136/300\n",
      "2025-03-16 20:29:40,691 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.083580\n",
      "2025-03-16 20:29:40,691 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.094490\n",
      "2025-03-16 20:29:40,891 - sciml.model.deeponet.deeponet - INFO - Epoch 137/300\n",
      "2025-03-16 20:29:40,892 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.083627\n",
      "2025-03-16 20:29:40,893 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.096180\n",
      "Training progress:  46%|████▌     | 137/300 [00:25<00:18,  8.82it/s]2025-03-16 20:29:41,095 - sciml.model.deeponet.deeponet - INFO - Epoch 138/300\n",
      "2025-03-16 20:29:41,096 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.083446\n",
      "2025-03-16 20:29:41,097 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.094343\n",
      "2025-03-16 20:29:41,305 - sciml.model.deeponet.deeponet - INFO - Epoch 139/300\n",
      "2025-03-16 20:29:41,305 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.083028\n",
      "2025-03-16 20:29:41,306 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.094498\n",
      "2025-03-16 20:29:41,501 - sciml.model.deeponet.deeponet - INFO - Epoch 140/300\n",
      "2025-03-16 20:29:41,502 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082796\n",
      "2025-03-16 20:29:41,502 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.095292\n",
      "Training progress:  47%|████▋     | 140/300 [00:26<00:21,  7.55it/s]2025-03-16 20:29:41,708 - sciml.model.deeponet.deeponet - INFO - Epoch 141/300\n",
      "2025-03-16 20:29:41,709 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082907\n",
      "2025-03-16 20:29:41,709 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.093672\n",
      "2025-03-16 20:29:41,914 - sciml.model.deeponet.deeponet - INFO - Epoch 142/300\n",
      "2025-03-16 20:29:41,915 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.083028\n",
      "2025-03-16 20:29:41,916 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.095608\n",
      "Training progress:  47%|████▋     | 142/300 [00:26<00:22,  6.91it/s]2025-03-16 20:29:42,132 - sciml.model.deeponet.deeponet - INFO - Epoch 143/300\n",
      "2025-03-16 20:29:42,133 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.083072\n",
      "2025-03-16 20:29:42,134 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.093840\n",
      "2025-03-16 20:29:42,343 - sciml.model.deeponet.deeponet - INFO - Epoch 144/300\n",
      "2025-03-16 20:29:42,344 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082746\n",
      "2025-03-16 20:29:42,345 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.094043\n",
      "Training progress:  48%|████▊     | 144/300 [00:27<00:24,  6.32it/s]2025-03-16 20:29:42,547 - sciml.model.deeponet.deeponet - INFO - Epoch 145/300\n",
      "2025-03-16 20:29:42,548 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082370\n",
      "2025-03-16 20:29:42,548 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.093469\n",
      "2025-03-16 20:29:42,747 - sciml.model.deeponet.deeponet - INFO - Epoch 146/300\n",
      "2025-03-16 20:29:42,747 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082195\n",
      "2025-03-16 20:29:42,748 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.093492\n",
      "Training progress:  49%|████▊     | 146/300 [00:27<00:25,  5.96it/s]2025-03-16 20:29:42,951 - sciml.model.deeponet.deeponet - INFO - Epoch 147/300\n",
      "2025-03-16 20:29:42,951 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082165\n",
      "2025-03-16 20:29:42,952 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.094410\n",
      "Training progress:  49%|████▉     | 147/300 [00:27<00:26,  5.80it/s]2025-03-16 20:29:43,155 - sciml.model.deeponet.deeponet - INFO - Epoch 148/300\n",
      "2025-03-16 20:29:43,156 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082296\n",
      "2025-03-16 20:29:43,156 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092760\n",
      "Training progress:  49%|████▉     | 148/300 [00:27<00:26,  5.63it/s]2025-03-16 20:29:43,359 - sciml.model.deeponet.deeponet - INFO - Epoch 149/300\n",
      "2025-03-16 20:29:43,359 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082290\n",
      "2025-03-16 20:29:43,360 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.094496\n",
      "Training progress:  50%|████▉     | 149/300 [00:28<00:27,  5.48it/s]2025-03-16 20:29:43,560 - sciml.model.deeponet.deeponet - INFO - Epoch 150/300\n",
      "2025-03-16 20:29:43,561 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082384\n",
      "2025-03-16 20:29:43,562 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092444\n",
      "Training progress:  50%|█████     | 150/300 [00:28<00:27,  5.36it/s]2025-03-16 20:29:43,756 - sciml.model.deeponet.deeponet - INFO - Epoch 151/300\n",
      "2025-03-16 20:29:43,757 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082188\n",
      "2025-03-16 20:29:43,757 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.094295\n",
      "Training progress:  50%|█████     | 151/300 [00:28<00:28,  5.30it/s]2025-03-16 20:29:43,960 - sciml.model.deeponet.deeponet - INFO - Epoch 152/300\n",
      "2025-03-16 20:29:43,961 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082091\n",
      "2025-03-16 20:29:43,961 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092875\n",
      "Training progress:  51%|█████     | 152/300 [00:28<00:28,  5.20it/s]2025-03-16 20:29:44,163 - sciml.model.deeponet.deeponet - INFO - Epoch 153/300\n",
      "2025-03-16 20:29:44,164 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.081820\n",
      "2025-03-16 20:29:44,164 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.093106\n",
      "Training progress:  51%|█████     | 153/300 [00:28<00:28,  5.12it/s]2025-03-16 20:29:44,365 - sciml.model.deeponet.deeponet - INFO - Epoch 154/300\n",
      "2025-03-16 20:29:44,366 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.081591\n",
      "2025-03-16 20:29:44,367 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092508\n",
      "Training progress:  51%|█████▏    | 154/300 [00:29<00:28,  5.07it/s]2025-03-16 20:29:44,570 - sciml.model.deeponet.deeponet - INFO - Epoch 155/300\n",
      "2025-03-16 20:29:44,571 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.081422\n",
      "2025-03-16 20:29:44,572 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092898\n",
      "Training progress:  52%|█████▏    | 155/300 [00:29<00:28,  5.02it/s]2025-03-16 20:29:44,771 - sciml.model.deeponet.deeponet - INFO - Epoch 156/300\n",
      "2025-03-16 20:29:44,772 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.081285\n",
      "2025-03-16 20:29:44,773 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092604\n",
      "Training progress:  52%|█████▏    | 156/300 [00:29<00:28,  5.00it/s]2025-03-16 20:29:44,974 - sciml.model.deeponet.deeponet - INFO - Epoch 157/300\n",
      "2025-03-16 20:29:44,975 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.081217\n",
      "2025-03-16 20:29:44,975 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092316\n",
      "Training progress:  52%|█████▏    | 157/300 [00:29<00:28,  4.98it/s]2025-03-16 20:29:45,180 - sciml.model.deeponet.deeponet - INFO - Epoch 158/300\n",
      "2025-03-16 20:29:45,181 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.081114\n",
      "2025-03-16 20:29:45,181 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091998\n",
      "Training progress:  53%|█████▎    | 158/300 [00:29<00:28,  4.95it/s]2025-03-16 20:29:45,382 - sciml.model.deeponet.deeponet - INFO - Epoch 159/300\n",
      "2025-03-16 20:29:45,383 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.081042\n",
      "2025-03-16 20:29:45,384 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092329\n",
      "Training progress:  53%|█████▎    | 159/300 [00:30<00:28,  4.95it/s]2025-03-16 20:29:45,588 - sciml.model.deeponet.deeponet - INFO - Epoch 160/300\n",
      "2025-03-16 20:29:45,589 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080949\n",
      "2025-03-16 20:29:45,589 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092245\n",
      "Training progress:  53%|█████▎    | 160/300 [00:30<00:28,  4.92it/s]2025-03-16 20:29:45,788 - sciml.model.deeponet.deeponet - INFO - Epoch 161/300\n",
      "2025-03-16 20:29:45,789 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080867\n",
      "2025-03-16 20:29:45,789 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091905\n",
      "Training progress:  54%|█████▎    | 161/300 [00:30<00:28,  4.94it/s]2025-03-16 20:29:45,985 - sciml.model.deeponet.deeponet - INFO - Epoch 162/300\n",
      "2025-03-16 20:29:45,986 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080792\n",
      "2025-03-16 20:29:45,986 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091912\n",
      "Training progress:  54%|█████▍    | 162/300 [00:30<00:27,  4.98it/s]2025-03-16 20:29:46,188 - sciml.model.deeponet.deeponet - INFO - Epoch 163/300\n",
      "2025-03-16 20:29:46,188 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080703\n",
      "2025-03-16 20:29:46,189 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091799\n",
      "Training progress:  54%|█████▍    | 163/300 [00:30<00:27,  4.97it/s]2025-03-16 20:29:46,389 - sciml.model.deeponet.deeponet - INFO - Epoch 164/300\n",
      "2025-03-16 20:29:46,389 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080641\n",
      "2025-03-16 20:29:46,390 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092184\n",
      "Training progress:  55%|█████▍    | 164/300 [00:31<00:27,  4.97it/s]2025-03-16 20:29:46,593 - sciml.model.deeponet.deeponet - INFO - Epoch 165/300\n",
      "2025-03-16 20:29:46,594 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080586\n",
      "2025-03-16 20:29:46,594 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091099\n",
      "Training progress:  55%|█████▌    | 165/300 [00:31<00:27,  4.95it/s]2025-03-16 20:29:46,793 - sciml.model.deeponet.deeponet - INFO - Epoch 166/300\n",
      "2025-03-16 20:29:46,793 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080683\n",
      "2025-03-16 20:29:46,794 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.093463\n",
      "Training progress:  55%|█████▌    | 166/300 [00:31<00:26,  4.96it/s]2025-03-16 20:29:46,996 - sciml.model.deeponet.deeponet - INFO - Epoch 167/300\n",
      "2025-03-16 20:29:46,996 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.081206\n",
      "2025-03-16 20:29:46,997 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091827\n",
      "Training progress:  56%|█████▌    | 167/300 [00:31<00:26,  4.96it/s]2025-03-16 20:29:47,197 - sciml.model.deeponet.deeponet - INFO - Epoch 168/300\n",
      "2025-03-16 20:29:47,198 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.083049\n",
      "2025-03-16 20:29:47,207 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.101865\n",
      "Training progress:  56%|█████▌    | 168/300 [00:31<00:27,  4.89it/s]2025-03-16 20:29:47,407 - sciml.model.deeponet.deeponet - INFO - Epoch 169/300\n",
      "2025-03-16 20:29:47,408 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.089035\n",
      "2025-03-16 20:29:47,408 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.093050\n",
      "Training progress:  56%|█████▋    | 169/300 [00:32<00:26,  4.92it/s]2025-03-16 20:29:47,608 - sciml.model.deeponet.deeponet - INFO - Epoch 170/300\n",
      "2025-03-16 20:29:47,609 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.085889\n",
      "2025-03-16 20:29:47,610 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.093849\n",
      "Training progress:  57%|█████▋    | 170/300 [00:32<00:26,  4.93it/s]2025-03-16 20:29:47,814 - sciml.model.deeponet.deeponet - INFO - Epoch 171/300\n",
      "2025-03-16 20:29:47,815 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082166\n",
      "2025-03-16 20:29:47,815 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092300\n",
      "Training progress:  57%|█████▋    | 171/300 [00:32<00:26,  4.91it/s]2025-03-16 20:29:48,016 - sciml.model.deeponet.deeponet - INFO - Epoch 172/300\n",
      "2025-03-16 20:29:48,017 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080679\n",
      "2025-03-16 20:29:48,018 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091793\n",
      "Training progress:  57%|█████▋    | 172/300 [00:32<00:26,  4.92it/s]2025-03-16 20:29:48,221 - sciml.model.deeponet.deeponet - INFO - Epoch 173/300\n",
      "2025-03-16 20:29:48,222 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.084061\n",
      "2025-03-16 20:29:48,222 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.096469\n",
      "Training progress:  58%|█████▊    | 173/300 [00:32<00:25,  4.91it/s]2025-03-16 20:29:48,427 - sciml.model.deeponet.deeponet - INFO - Epoch 174/300\n",
      "2025-03-16 20:29:48,428 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.084287\n",
      "2025-03-16 20:29:48,428 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091148\n",
      "Training progress:  58%|█████▊    | 174/300 [00:33<00:25,  4.89it/s]2025-03-16 20:29:48,629 - sciml.model.deeponet.deeponet - INFO - Epoch 175/300\n",
      "2025-03-16 20:29:48,629 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080195\n",
      "2025-03-16 20:29:48,630 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092688\n",
      "Training progress:  58%|█████▊    | 175/300 [00:33<00:25,  4.91it/s]2025-03-16 20:29:48,828 - sciml.model.deeponet.deeponet - INFO - Epoch 176/300\n",
      "2025-03-16 20:29:48,829 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.084680\n",
      "2025-03-16 20:29:48,829 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.097970\n",
      "Training progress:  59%|█████▊    | 176/300 [00:33<00:25,  4.94it/s]2025-03-16 20:29:49,033 - sciml.model.deeponet.deeponet - INFO - Epoch 177/300\n",
      "2025-03-16 20:29:49,034 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.085119\n",
      "2025-03-16 20:29:49,034 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092985\n",
      "Training progress:  59%|█████▉    | 177/300 [00:33<00:24,  4.92it/s]2025-03-16 20:29:49,235 - sciml.model.deeponet.deeponet - INFO - Epoch 178/300\n",
      "2025-03-16 20:29:49,236 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.081234\n",
      "2025-03-16 20:29:49,237 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.093391\n",
      "Training progress:  59%|█████▉    | 178/300 [00:33<00:24,  4.93it/s]2025-03-16 20:29:49,436 - sciml.model.deeponet.deeponet - INFO - Epoch 179/300\n",
      "2025-03-16 20:29:49,437 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.086252\n",
      "2025-03-16 20:29:49,438 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092912\n",
      "Training progress:  60%|█████▉    | 179/300 [00:34<00:24,  4.94it/s]2025-03-16 20:29:49,638 - sciml.model.deeponet.deeponet - INFO - Epoch 180/300\n",
      "2025-03-16 20:29:49,639 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.081587\n",
      "2025-03-16 20:29:49,640 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.093802\n",
      "Training progress:  60%|██████    | 180/300 [00:34<00:24,  4.95it/s]2025-03-16 20:29:49,844 - sciml.model.deeponet.deeponet - INFO - Epoch 181/300\n",
      "2025-03-16 20:29:49,845 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082679\n",
      "2025-03-16 20:29:49,846 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092202\n",
      "Training progress:  60%|██████    | 181/300 [00:34<00:24,  4.92it/s]2025-03-16 20:29:50,049 - sciml.model.deeponet.deeponet - INFO - Epoch 182/300\n",
      "2025-03-16 20:29:50,049 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.082893\n",
      "2025-03-16 20:29:50,050 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092247\n",
      "Training progress:  61%|██████    | 182/300 [00:34<00:24,  4.91it/s]2025-03-16 20:29:50,248 - sciml.model.deeponet.deeponet - INFO - Epoch 183/300\n",
      "2025-03-16 20:29:50,249 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080320\n",
      "2025-03-16 20:29:50,250 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.093572\n",
      "Training progress:  61%|██████    | 183/300 [00:34<00:23,  4.94it/s]2025-03-16 20:29:50,451 - sciml.model.deeponet.deeponet - INFO - Epoch 184/300\n",
      "2025-03-16 20:29:50,451 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.081869\n",
      "2025-03-16 20:29:50,452 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090578\n",
      "Training progress:  61%|██████▏   | 184/300 [00:35<00:23,  4.94it/s]2025-03-16 20:29:50,652 - sciml.model.deeponet.deeponet - INFO - Epoch 185/300\n",
      "2025-03-16 20:29:50,652 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080142\n",
      "2025-03-16 20:29:50,653 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090857\n",
      "Training progress:  62%|██████▏   | 185/300 [00:35<00:23,  4.95it/s]2025-03-16 20:29:50,855 - sciml.model.deeponet.deeponet - INFO - Epoch 186/300\n",
      "2025-03-16 20:29:50,856 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.081557\n",
      "2025-03-16 20:29:50,857 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090692\n",
      "Training progress:  62%|██████▏   | 186/300 [00:35<00:23,  4.94it/s]2025-03-16 20:29:51,056 - sciml.model.deeponet.deeponet - INFO - Epoch 187/300\n",
      "2025-03-16 20:29:51,057 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080416\n",
      "2025-03-16 20:29:51,058 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091524\n",
      "Training progress:  62%|██████▏   | 187/300 [00:35<00:22,  4.94it/s]2025-03-16 20:29:51,261 - sciml.model.deeponet.deeponet - INFO - Epoch 188/300\n",
      "2025-03-16 20:29:51,261 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080384\n",
      "2025-03-16 20:29:51,262 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092022\n",
      "Training progress:  63%|██████▎   | 188/300 [00:36<00:22,  4.93it/s]2025-03-16 20:29:51,460 - sciml.model.deeponet.deeponet - INFO - Epoch 189/300\n",
      "2025-03-16 20:29:51,461 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080703\n",
      "2025-03-16 20:29:51,462 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091438\n",
      "Training progress:  63%|██████▎   | 189/300 [00:36<00:22,  4.95it/s]2025-03-16 20:29:51,664 - sciml.model.deeponet.deeponet - INFO - Epoch 190/300\n",
      "2025-03-16 20:29:51,665 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079835\n",
      "2025-03-16 20:29:51,665 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.092096\n",
      "Training progress:  63%|██████▎   | 190/300 [00:36<00:22,  4.94it/s]2025-03-16 20:29:51,864 - sciml.model.deeponet.deeponet - INFO - Epoch 191/300\n",
      "2025-03-16 20:29:51,865 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080298\n",
      "2025-03-16 20:29:51,866 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090779\n",
      "Training progress:  64%|██████▎   | 191/300 [00:36<00:21,  4.96it/s]2025-03-16 20:29:52,072 - sciml.model.deeponet.deeponet - INFO - Epoch 192/300\n",
      "2025-03-16 20:29:52,073 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079607\n",
      "2025-03-16 20:29:52,073 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090278\n",
      "Training progress:  64%|██████▍   | 192/300 [00:36<00:21,  4.91it/s]2025-03-16 20:29:52,275 - sciml.model.deeponet.deeponet - INFO - Epoch 193/300\n",
      "2025-03-16 20:29:52,275 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.080229\n",
      "2025-03-16 20:29:52,276 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090597\n",
      "Training progress:  64%|██████▍   | 193/300 [00:37<00:21,  4.92it/s]2025-03-16 20:29:52,477 - sciml.model.deeponet.deeponet - INFO - Epoch 194/300\n",
      "2025-03-16 20:29:52,478 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079589\n",
      "2025-03-16 20:29:52,478 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091480\n",
      "Training progress:  65%|██████▍   | 194/300 [00:37<00:21,  4.93it/s]2025-03-16 20:29:52,679 - sciml.model.deeponet.deeponet - INFO - Epoch 195/300\n",
      "2025-03-16 20:29:52,680 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079690\n",
      "2025-03-16 20:29:52,681 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090415\n",
      "Training progress:  65%|██████▌   | 195/300 [00:37<00:21,  4.93it/s]2025-03-16 20:29:52,882 - sciml.model.deeponet.deeponet - INFO - Epoch 196/300\n",
      "2025-03-16 20:29:52,883 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079777\n",
      "2025-03-16 20:29:52,883 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090029\n",
      "Training progress:  65%|██████▌   | 196/300 [00:37<00:21,  4.93it/s]2025-03-16 20:29:53,085 - sciml.model.deeponet.deeponet - INFO - Epoch 197/300\n",
      "2025-03-16 20:29:53,086 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079279\n",
      "2025-03-16 20:29:53,087 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091111\n",
      "Training progress:  66%|██████▌   | 197/300 [00:37<00:20,  4.93it/s]2025-03-16 20:29:53,289 - sciml.model.deeponet.deeponet - INFO - Epoch 198/300\n",
      "2025-03-16 20:29:53,289 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079617\n",
      "2025-03-16 20:29:53,290 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089878\n",
      "Training progress:  66%|██████▌   | 198/300 [00:38<00:20,  4.93it/s]2025-03-16 20:29:53,496 - sciml.model.deeponet.deeponet - INFO - Epoch 199/300\n",
      "2025-03-16 20:29:53,496 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079173\n",
      "2025-03-16 20:29:53,497 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090056\n",
      "Training progress:  66%|██████▋   | 199/300 [00:38<00:20,  4.90it/s]2025-03-16 20:29:53,697 - sciml.model.deeponet.deeponet - INFO - Epoch 200/300\n",
      "2025-03-16 20:29:53,698 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079406\n",
      "2025-03-16 20:29:53,698 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090870\n",
      "Training progress:  67%|██████▋   | 200/300 [00:38<00:20,  4.91it/s]2025-03-16 20:29:53,900 - sciml.model.deeponet.deeponet - INFO - Epoch 201/300\n",
      "2025-03-16 20:29:53,901 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079258\n",
      "2025-03-16 20:29:53,902 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090090\n",
      "Training progress:  67%|██████▋   | 201/300 [00:38<00:20,  4.92it/s]2025-03-16 20:29:54,106 - sciml.model.deeponet.deeponet - INFO - Epoch 202/300\n",
      "2025-03-16 20:29:54,107 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079059\n",
      "2025-03-16 20:29:54,107 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089495\n",
      "Training progress:  67%|██████▋   | 202/300 [00:38<00:19,  4.90it/s]2025-03-16 20:29:54,309 - sciml.model.deeponet.deeponet - INFO - Epoch 203/300\n",
      "2025-03-16 20:29:54,310 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079275\n",
      "2025-03-16 20:29:54,310 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089976\n",
      "Training progress:  68%|██████▊   | 203/300 [00:39<00:19,  4.91it/s]2025-03-16 20:29:54,516 - sciml.model.deeponet.deeponet - INFO - Epoch 204/300\n",
      "2025-03-16 20:29:54,516 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078940\n",
      "2025-03-16 20:29:54,517 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089984\n",
      "Training progress:  68%|██████▊   | 204/300 [00:39<00:19,  4.89it/s]2025-03-16 20:29:54,719 - sciml.model.deeponet.deeponet - INFO - Epoch 205/300\n",
      "2025-03-16 20:29:54,720 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079070\n",
      "2025-03-16 20:29:54,720 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090203\n",
      "Training progress:  68%|██████▊   | 205/300 [00:39<00:19,  4.89it/s]2025-03-16 20:29:54,922 - sciml.model.deeponet.deeponet - INFO - Epoch 206/300\n",
      "2025-03-16 20:29:54,923 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078975\n",
      "2025-03-16 20:29:54,923 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089576\n",
      "Training progress:  69%|██████▊   | 206/300 [00:39<00:19,  4.91it/s]2025-03-16 20:29:55,122 - sciml.model.deeponet.deeponet - INFO - Epoch 207/300\n",
      "2025-03-16 20:29:55,123 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078844\n",
      "2025-03-16 20:29:55,124 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090416\n",
      "Training progress:  69%|██████▉   | 207/300 [00:39<00:18,  4.93it/s]2025-03-16 20:29:55,322 - sciml.model.deeponet.deeponet - INFO - Epoch 208/300\n",
      "2025-03-16 20:29:55,323 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078897\n",
      "2025-03-16 20:29:55,324 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089582\n",
      "Training progress:  69%|██████▉   | 208/300 [00:40<00:18,  4.95it/s]2025-03-16 20:29:55,525 - sciml.model.deeponet.deeponet - INFO - Epoch 209/300\n",
      "2025-03-16 20:29:55,526 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078671\n",
      "2025-03-16 20:29:55,526 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089207\n",
      "Training progress:  70%|██████▉   | 209/300 [00:40<00:18,  4.95it/s]2025-03-16 20:29:55,729 - sciml.model.deeponet.deeponet - INFO - Epoch 210/300\n",
      "2025-03-16 20:29:55,729 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078723\n",
      "2025-03-16 20:29:55,730 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090326\n",
      "Training progress:  70%|███████   | 210/300 [00:40<00:18,  4.94it/s]2025-03-16 20:29:55,927 - sciml.model.deeponet.deeponet - INFO - Epoch 211/300\n",
      "2025-03-16 20:29:55,928 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078759\n",
      "2025-03-16 20:29:55,929 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089156\n",
      "Training progress:  70%|███████   | 211/300 [00:40<00:17,  4.96it/s]2025-03-16 20:29:56,130 - sciml.model.deeponet.deeponet - INFO - Epoch 212/300\n",
      "2025-03-16 20:29:56,131 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078620\n",
      "2025-03-16 20:29:56,132 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089623\n",
      "Training progress:  71%|███████   | 212/300 [00:40<00:17,  4.95it/s]2025-03-16 20:29:56,377 - sciml.model.deeponet.deeponet - INFO - Epoch 213/300\n",
      "2025-03-16 20:29:56,378 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078700\n",
      "2025-03-16 20:29:56,379 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089258\n",
      "Training progress:  71%|███████   | 213/300 [00:41<00:18,  4.64it/s]2025-03-16 20:29:56,595 - sciml.model.deeponet.deeponet - INFO - Epoch 214/300\n",
      "2025-03-16 20:29:56,596 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078616\n",
      "2025-03-16 20:29:56,596 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089717\n",
      "Training progress:  71%|███████▏  | 214/300 [00:41<00:18,  4.63it/s]2025-03-16 20:29:56,801 - sciml.model.deeponet.deeponet - INFO - Epoch 215/300\n",
      "2025-03-16 20:29:56,802 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078438\n",
      "2025-03-16 20:29:56,803 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089400\n",
      "Training progress:  72%|███████▏  | 215/300 [00:41<00:18,  4.69it/s]2025-03-16 20:29:57,004 - sciml.model.deeponet.deeponet - INFO - Epoch 216/300\n",
      "2025-03-16 20:29:57,005 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078494\n",
      "2025-03-16 20:29:57,005 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089418\n",
      "Training progress:  72%|███████▏  | 216/300 [00:41<00:17,  4.76it/s]2025-03-16 20:29:57,209 - sciml.model.deeponet.deeponet - INFO - Epoch 217/300\n",
      "2025-03-16 20:29:57,209 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078466\n",
      "2025-03-16 20:29:57,210 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089897\n",
      "Training progress:  72%|███████▏  | 217/300 [00:41<00:17,  4.80it/s]2025-03-16 20:29:57,410 - sciml.model.deeponet.deeponet - INFO - Epoch 218/300\n",
      "2025-03-16 20:29:57,411 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078458\n",
      "2025-03-16 20:29:57,421 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088542\n",
      "Training progress:  73%|███████▎  | 218/300 [00:42<00:17,  4.78it/s]2025-03-16 20:29:57,625 - sciml.model.deeponet.deeponet - INFO - Epoch 219/300\n",
      "2025-03-16 20:29:57,626 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078529\n",
      "2025-03-16 20:29:57,626 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090001\n",
      "Training progress:  73%|███████▎  | 219/300 [00:42<00:16,  4.81it/s]2025-03-16 20:29:57,829 - sciml.model.deeponet.deeponet - INFO - Epoch 220/300\n",
      "2025-03-16 20:29:57,830 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078372\n",
      "2025-03-16 20:29:57,830 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089081\n",
      "Training progress:  73%|███████▎  | 220/300 [00:42<00:16,  4.83it/s]2025-03-16 20:29:58,030 - sciml.model.deeponet.deeponet - INFO - Epoch 221/300\n",
      "2025-03-16 20:29:58,030 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078244\n",
      "2025-03-16 20:29:58,031 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089193\n",
      "Training progress:  74%|███████▎  | 221/300 [00:42<00:16,  4.88it/s]2025-03-16 20:29:58,236 - sciml.model.deeponet.deeponet - INFO - Epoch 222/300\n",
      "2025-03-16 20:29:58,237 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078240\n",
      "2025-03-16 20:29:58,237 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089845\n",
      "Training progress:  74%|███████▍  | 222/300 [00:42<00:16,  4.87it/s]2025-03-16 20:29:58,445 - sciml.model.deeponet.deeponet - INFO - Epoch 223/300\n",
      "2025-03-16 20:29:58,446 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078221\n",
      "2025-03-16 20:29:58,447 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088684\n",
      "Training progress:  74%|███████▍  | 223/300 [00:43<00:15,  4.84it/s]2025-03-16 20:29:58,646 - sciml.model.deeponet.deeponet - INFO - Epoch 224/300\n",
      "2025-03-16 20:29:58,647 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078253\n",
      "2025-03-16 20:29:58,648 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089668\n",
      "Training progress:  75%|███████▍  | 224/300 [00:43<00:15,  4.88it/s]2025-03-16 20:29:58,851 - sciml.model.deeponet.deeponet - INFO - Epoch 225/300\n",
      "2025-03-16 20:29:58,851 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078375\n",
      "2025-03-16 20:29:58,852 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089021\n",
      "Training progress:  75%|███████▌  | 225/300 [00:43<00:15,  4.89it/s]2025-03-16 20:29:59,050 - sciml.model.deeponet.deeponet - INFO - Epoch 226/300\n",
      "2025-03-16 20:29:59,051 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078167\n",
      "2025-03-16 20:29:59,052 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089179\n",
      "Training progress:  75%|███████▌  | 226/300 [00:43<00:15,  4.92it/s]2025-03-16 20:29:59,256 - sciml.model.deeponet.deeponet - INFO - Epoch 227/300\n",
      "2025-03-16 20:29:59,257 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077961\n",
      "2025-03-16 20:29:59,257 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089371\n",
      "Training progress:  76%|███████▌  | 227/300 [00:44<00:14,  4.90it/s]2025-03-16 20:29:59,461 - sciml.model.deeponet.deeponet - INFO - Epoch 228/300\n",
      "2025-03-16 20:29:59,462 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078004\n",
      "2025-03-16 20:29:59,462 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088830\n",
      "Training progress:  76%|███████▌  | 228/300 [00:44<00:14,  4.89it/s]2025-03-16 20:29:59,665 - sciml.model.deeponet.deeponet - INFO - Epoch 229/300\n",
      "2025-03-16 20:29:59,666 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078191\n",
      "2025-03-16 20:29:59,667 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089438\n",
      "Training progress:  76%|███████▋  | 229/300 [00:44<00:14,  4.90it/s]2025-03-16 20:29:59,867 - sciml.model.deeponet.deeponet - INFO - Epoch 230/300\n",
      "2025-03-16 20:29:59,868 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078118\n",
      "2025-03-16 20:29:59,869 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088633\n",
      "Training progress:  77%|███████▋  | 230/300 [00:44<00:14,  4.91it/s]2025-03-16 20:30:00,068 - sciml.model.deeponet.deeponet - INFO - Epoch 231/300\n",
      "2025-03-16 20:30:00,069 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077889\n",
      "2025-03-16 20:30:00,070 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089231\n",
      "Training progress:  77%|███████▋  | 231/300 [00:44<00:13,  4.93it/s]2025-03-16 20:30:00,281 - sciml.model.deeponet.deeponet - INFO - Epoch 232/300\n",
      "2025-03-16 20:30:00,282 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077835\n",
      "2025-03-16 20:30:00,283 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089124\n",
      "Training progress:  77%|███████▋  | 232/300 [00:45<00:14,  4.85it/s]2025-03-16 20:30:00,576 - sciml.model.deeponet.deeponet - INFO - Epoch 233/300\n",
      "2025-03-16 20:30:00,577 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077857\n",
      "2025-03-16 20:30:00,578 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088613\n",
      "Training progress:  78%|███████▊  | 233/300 [00:45<00:15,  4.30it/s]2025-03-16 20:30:00,819 - sciml.model.deeponet.deeponet - INFO - Epoch 234/300\n",
      "2025-03-16 20:30:00,820 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077903\n",
      "2025-03-16 20:30:00,820 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089442\n",
      "Training progress:  78%|███████▊  | 234/300 [00:45<00:15,  4.24it/s]2025-03-16 20:30:01,026 - sciml.model.deeponet.deeponet - INFO - Epoch 235/300\n",
      "2025-03-16 20:30:01,027 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077868\n",
      "2025-03-16 20:30:01,027 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088390\n",
      "Training progress:  78%|███████▊  | 235/300 [00:45<00:14,  4.41it/s]2025-03-16 20:30:01,234 - sciml.model.deeponet.deeponet - INFO - Epoch 236/300\n",
      "2025-03-16 20:30:01,235 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077785\n",
      "2025-03-16 20:30:01,235 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089203\n",
      "Training progress:  79%|███████▊  | 236/300 [00:45<00:14,  4.52it/s]2025-03-16 20:30:01,437 - sciml.model.deeponet.deeponet - INFO - Epoch 237/300\n",
      "2025-03-16 20:30:01,437 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077677\n",
      "2025-03-16 20:30:01,438 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088769\n",
      "Training progress:  79%|███████▉  | 237/300 [00:46<00:13,  4.64it/s]2025-03-16 20:30:01,645 - sciml.model.deeponet.deeponet - INFO - Epoch 238/300\n",
      "2025-03-16 20:30:01,646 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077609\n",
      "2025-03-16 20:30:01,646 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088603\n",
      "Training progress:  79%|███████▉  | 238/300 [00:46<00:13,  4.69it/s]2025-03-16 20:30:01,854 - sciml.model.deeponet.deeponet - INFO - Epoch 239/300\n",
      "2025-03-16 20:30:01,855 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077585\n",
      "2025-03-16 20:30:01,856 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089257\n",
      "Training progress:  80%|███████▉  | 239/300 [00:46<00:12,  4.71it/s]2025-03-16 20:30:02,060 - sciml.model.deeponet.deeponet - INFO - Epoch 240/300\n",
      "2025-03-16 20:30:02,061 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077655\n",
      "2025-03-16 20:30:02,062 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088193\n",
      "Training progress:  80%|████████  | 240/300 [00:46<00:12,  4.75it/s]2025-03-16 20:30:02,272 - sciml.model.deeponet.deeponet - INFO - Epoch 241/300\n",
      "2025-03-16 20:30:02,273 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077745\n",
      "2025-03-16 20:30:02,274 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089336\n",
      "Training progress:  80%|████████  | 241/300 [00:47<00:12,  4.74it/s]2025-03-16 20:30:02,482 - sciml.model.deeponet.deeponet - INFO - Epoch 242/300\n",
      "2025-03-16 20:30:02,483 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077723\n",
      "2025-03-16 20:30:02,484 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088196\n",
      "Training progress:  81%|████████  | 242/300 [00:47<00:12,  4.75it/s]2025-03-16 20:30:02,693 - sciml.model.deeponet.deeponet - INFO - Epoch 243/300\n",
      "2025-03-16 20:30:02,694 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077533\n",
      "2025-03-16 20:30:02,695 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088701\n",
      "Training progress:  81%|████████  | 243/300 [00:47<00:12,  4.74it/s]2025-03-16 20:30:02,902 - sciml.model.deeponet.deeponet - INFO - Epoch 244/300\n",
      "2025-03-16 20:30:02,903 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077417\n",
      "2025-03-16 20:30:02,904 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088618\n",
      "Training progress:  81%|████████▏ | 244/300 [00:47<00:11,  4.75it/s]2025-03-16 20:30:03,109 - sciml.model.deeponet.deeponet - INFO - Epoch 245/300\n",
      "2025-03-16 20:30:03,110 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077377\n",
      "2025-03-16 20:30:03,111 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088478\n",
      "Training progress:  82%|████████▏ | 245/300 [00:47<00:11,  4.78it/s]2025-03-16 20:30:03,319 - sciml.model.deeponet.deeponet - INFO - Epoch 246/300\n",
      "2025-03-16 20:30:03,320 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077375\n",
      "2025-03-16 20:30:03,320 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088790\n",
      "Training progress:  82%|████████▏ | 246/300 [00:48<00:11,  4.78it/s]2025-03-16 20:30:03,527 - sciml.model.deeponet.deeponet - INFO - Epoch 247/300\n",
      "2025-03-16 20:30:03,528 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077380\n",
      "2025-03-16 20:30:03,529 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088442\n",
      "Training progress:  82%|████████▏ | 247/300 [00:48<00:11,  4.78it/s]2025-03-16 20:30:03,735 - sciml.model.deeponet.deeponet - INFO - Epoch 248/300\n",
      "2025-03-16 20:30:03,736 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077354\n",
      "2025-03-16 20:30:03,737 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088505\n",
      "Training progress:  83%|████████▎ | 248/300 [00:48<00:10,  4.79it/s]2025-03-16 20:30:03,944 - sciml.model.deeponet.deeponet - INFO - Epoch 249/300\n",
      "2025-03-16 20:30:03,945 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077331\n",
      "2025-03-16 20:30:03,946 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088681\n",
      "Training progress:  83%|████████▎ | 249/300 [00:48<00:10,  4.79it/s]2025-03-16 20:30:04,149 - sciml.model.deeponet.deeponet - INFO - Epoch 250/300\n",
      "2025-03-16 20:30:04,150 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077378\n",
      "2025-03-16 20:30:04,151 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088100\n",
      "Training progress:  83%|████████▎ | 250/300 [00:48<00:10,  4.81it/s]2025-03-16 20:30:04,360 - sciml.model.deeponet.deeponet - INFO - Epoch 251/300\n",
      "2025-03-16 20:30:04,361 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077383\n",
      "2025-03-16 20:30:04,362 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088932\n",
      "Training progress:  84%|████████▎ | 251/300 [00:49<00:10,  4.79it/s]2025-03-16 20:30:04,568 - sciml.model.deeponet.deeponet - INFO - Epoch 252/300\n",
      "2025-03-16 20:30:04,568 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077448\n",
      "2025-03-16 20:30:04,569 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088180\n",
      "Training progress:  84%|████████▍ | 252/300 [00:49<00:09,  4.80it/s]2025-03-16 20:30:04,782 - sciml.model.deeponet.deeponet - INFO - Epoch 253/300\n",
      "2025-03-16 20:30:04,783 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077219\n",
      "2025-03-16 20:30:04,784 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088254\n",
      "Training progress:  84%|████████▍ | 253/300 [00:49<00:09,  4.75it/s]2025-03-16 20:30:04,991 - sciml.model.deeponet.deeponet - INFO - Epoch 254/300\n",
      "2025-03-16 20:30:04,992 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077189\n",
      "2025-03-16 20:30:04,992 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088058\n",
      "Training progress:  85%|████████▍ | 254/300 [00:49<00:09,  4.77it/s]2025-03-16 20:30:05,200 - sciml.model.deeponet.deeponet - INFO - Epoch 255/300\n",
      "2025-03-16 20:30:05,201 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077349\n",
      "2025-03-16 20:30:05,202 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087956\n",
      "Training progress:  85%|████████▌ | 255/300 [00:49<00:09,  4.77it/s]2025-03-16 20:30:05,406 - sciml.model.deeponet.deeponet - INFO - Epoch 256/300\n",
      "2025-03-16 20:30:05,407 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077447\n",
      "2025-03-16 20:30:05,408 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088097\n",
      "Training progress:  85%|████████▌ | 256/300 [00:50<00:09,  4.80it/s]2025-03-16 20:30:05,614 - sciml.model.deeponet.deeponet - INFO - Epoch 257/300\n",
      "2025-03-16 20:30:05,614 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077401\n",
      "2025-03-16 20:30:05,624 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088282\n",
      "Training progress:  86%|████████▌ | 257/300 [00:50<00:09,  4.74it/s]2025-03-16 20:30:05,836 - sciml.model.deeponet.deeponet - INFO - Epoch 258/300\n",
      "2025-03-16 20:30:05,836 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077163\n",
      "2025-03-16 20:30:05,837 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087355\n",
      "Training progress:  86%|████████▌ | 258/300 [00:50<00:08,  4.73it/s]2025-03-16 20:30:06,042 - sciml.model.deeponet.deeponet - INFO - Epoch 259/300\n",
      "2025-03-16 20:30:06,043 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077153\n",
      "2025-03-16 20:30:06,043 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088886\n",
      "Training progress:  86%|████████▋ | 259/300 [00:50<00:08,  4.76it/s]2025-03-16 20:30:06,253 - sciml.model.deeponet.deeponet - INFO - Epoch 260/300\n",
      "2025-03-16 20:30:06,254 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077532\n",
      "2025-03-16 20:30:06,254 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087308\n",
      "Training progress:  87%|████████▋ | 260/300 [00:51<00:08,  4.76it/s]2025-03-16 20:30:06,459 - sciml.model.deeponet.deeponet - INFO - Epoch 261/300\n",
      "2025-03-16 20:30:06,460 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077085\n",
      "2025-03-16 20:30:06,460 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087945\n",
      "Training progress:  87%|████████▋ | 261/300 [00:51<00:08,  4.79it/s]2025-03-16 20:30:06,667 - sciml.model.deeponet.deeponet - INFO - Epoch 262/300\n",
      "2025-03-16 20:30:06,667 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.076974\n",
      "2025-03-16 20:30:06,668 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087851\n",
      "Training progress:  87%|████████▋ | 262/300 [00:51<00:07,  4.79it/s]2025-03-16 20:30:06,876 - sciml.model.deeponet.deeponet - INFO - Epoch 263/300\n",
      "2025-03-16 20:30:06,877 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077079\n",
      "2025-03-16 20:30:06,878 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087797\n",
      "Training progress:  88%|████████▊ | 263/300 [00:51<00:07,  4.78it/s]2025-03-16 20:30:07,077 - sciml.model.deeponet.deeponet - INFO - Epoch 264/300\n",
      "2025-03-16 20:30:07,078 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077095\n",
      "2025-03-16 20:30:07,078 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087802\n",
      "Training progress:  88%|████████▊ | 264/300 [00:51<00:07,  4.84it/s]2025-03-16 20:30:07,283 - sciml.model.deeponet.deeponet - INFO - Epoch 265/300\n",
      "2025-03-16 20:30:07,284 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077167\n",
      "2025-03-16 20:30:07,285 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088458\n",
      "Training progress:  88%|████████▊ | 265/300 [00:52<00:07,  4.85it/s]2025-03-16 20:30:07,493 - sciml.model.deeponet.deeponet - INFO - Epoch 266/300\n",
      "2025-03-16 20:30:07,494 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077211\n",
      "2025-03-16 20:30:07,494 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087273\n",
      "Training progress:  89%|████████▊ | 266/300 [00:52<00:07,  4.82it/s]2025-03-16 20:30:07,697 - sciml.model.deeponet.deeponet - INFO - Epoch 267/300\n",
      "2025-03-16 20:30:07,698 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077408\n",
      "2025-03-16 20:30:07,698 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089966\n",
      "Training progress:  89%|████████▉ | 267/300 [00:52<00:06,  4.85it/s]2025-03-16 20:30:07,903 - sciml.model.deeponet.deeponet - INFO - Epoch 268/300\n",
      "2025-03-16 20:30:07,904 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078160\n",
      "2025-03-16 20:30:07,904 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087296\n",
      "Training progress:  89%|████████▉ | 268/300 [00:52<00:06,  4.85it/s]2025-03-16 20:30:08,106 - sciml.model.deeponet.deeponet - INFO - Epoch 269/300\n",
      "2025-03-16 20:30:08,106 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078449\n",
      "2025-03-16 20:30:08,107 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091454\n",
      "Training progress:  90%|████████▉ | 269/300 [00:52<00:06,  4.88it/s]2025-03-16 20:30:08,311 - sciml.model.deeponet.deeponet - INFO - Epoch 270/300\n",
      "2025-03-16 20:30:08,312 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079400\n",
      "2025-03-16 20:30:08,312 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087260\n",
      "Training progress:  90%|█████████ | 270/300 [00:53<00:06,  4.87it/s]2025-03-16 20:30:05,585 - sciml.model.deeponet.deeponet - INFO - Epoch 271/300\n",
      "2025-03-16 20:30:05,586 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078221\n",
      "2025-03-16 20:30:05,587 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088505\n",
      "2025-03-16 20:30:05,789 - sciml.model.deeponet.deeponet - INFO - Epoch 272/300\n",
      "2025-03-16 20:30:05,790 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077518\n",
      "2025-03-16 20:30:05,790 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087505\n",
      "2025-03-16 20:30:05,993 - sciml.model.deeponet.deeponet - INFO - Epoch 273/300\n",
      "2025-03-16 20:30:05,994 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077038\n",
      "2025-03-16 20:30:05,994 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.086930\n",
      "2025-03-16 20:30:06,201 - sciml.model.deeponet.deeponet - INFO - Epoch 274/300\n",
      "2025-03-16 20:30:06,202 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.076870\n",
      "2025-03-16 20:30:06,202 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088822\n",
      "2025-03-16 20:30:06,404 - sciml.model.deeponet.deeponet - INFO - Epoch 275/300\n",
      "2025-03-16 20:30:06,404 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077433\n",
      "2025-03-16 20:30:06,405 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087642\n",
      "2025-03-16 20:30:06,610 - sciml.model.deeponet.deeponet - INFO - Epoch 276/300\n",
      "2025-03-16 20:30:06,611 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077913\n",
      "2025-03-16 20:30:06,612 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.089844\n",
      "2025-03-16 20:30:06,813 - sciml.model.deeponet.deeponet - INFO - Epoch 277/300\n",
      "2025-03-16 20:30:06,814 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078662\n",
      "2025-03-16 20:30:06,814 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087501\n",
      "2025-03-16 20:30:07,014 - sciml.model.deeponet.deeponet - INFO - Epoch 278/300\n",
      "2025-03-16 20:30:07,015 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077631\n",
      "2025-03-16 20:30:07,016 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087346\n",
      "2025-03-16 20:30:07,223 - sciml.model.deeponet.deeponet - INFO - Epoch 279/300\n",
      "2025-03-16 20:30:07,224 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.076677\n",
      "2025-03-16 20:30:07,225 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087292\n",
      "2025-03-16 20:30:07,426 - sciml.model.deeponet.deeponet - INFO - Epoch 280/300\n",
      "2025-03-16 20:30:07,427 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.076964\n",
      "2025-03-16 20:30:07,427 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088155\n",
      "2025-03-16 20:30:07,627 - sciml.model.deeponet.deeponet - INFO - Epoch 281/300\n",
      "2025-03-16 20:30:07,628 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077864\n",
      "2025-03-16 20:30:07,628 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.090227\n",
      "2025-03-16 20:30:07,830 - sciml.model.deeponet.deeponet - INFO - Epoch 282/300\n",
      "2025-03-16 20:30:07,831 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078467\n",
      "2025-03-16 20:30:07,840 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.086815\n",
      "2025-03-16 20:30:08,041 - sciml.model.deeponet.deeponet - INFO - Epoch 283/300\n",
      "2025-03-16 20:30:08,042 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077890\n",
      "2025-03-16 20:30:08,042 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088175\n",
      "2025-03-16 20:30:08,247 - sciml.model.deeponet.deeponet - INFO - Epoch 284/300\n",
      "2025-03-16 20:30:08,248 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077735\n",
      "2025-03-16 20:30:08,248 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087469\n",
      "2025-03-16 20:30:08,452 - sciml.model.deeponet.deeponet - INFO - Epoch 285/300\n",
      "2025-03-16 20:30:08,453 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.076956\n",
      "2025-03-16 20:30:08,453 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.086345\n",
      "Training progress:  95%|█████████▌| 285/300 [00:53<00:00, 27.98it/s]2025-03-16 20:30:08,660 - sciml.model.deeponet.deeponet - INFO - Epoch 286/300\n",
      "2025-03-16 20:30:08,661 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079316\n",
      "2025-03-16 20:30:08,662 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.091226\n",
      "2025-03-16 20:30:08,867 - sciml.model.deeponet.deeponet - INFO - Epoch 287/300\n",
      "2025-03-16 20:30:08,868 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.079404\n",
      "2025-03-16 20:30:08,869 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088990\n",
      "2025-03-16 20:30:09,084 - sciml.model.deeponet.deeponet - INFO - Epoch 288/300\n",
      "2025-03-16 20:30:09,085 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078264\n",
      "2025-03-16 20:30:09,085 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.086130\n",
      "2025-03-16 20:30:09,299 - sciml.model.deeponet.deeponet - INFO - Epoch 289/300\n",
      "2025-03-16 20:30:09,300 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077757\n",
      "2025-03-16 20:30:09,301 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087071\n",
      "Training progress:  96%|█████████▋| 289/300 [00:54<00:00, 12.59it/s]2025-03-16 20:30:09,509 - sciml.model.deeponet.deeponet - INFO - Epoch 290/300\n",
      "2025-03-16 20:30:09,510 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.076979\n",
      "2025-03-16 20:30:09,511 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087914\n",
      "2025-03-16 20:30:09,714 - sciml.model.deeponet.deeponet - INFO - Epoch 291/300\n",
      "2025-03-16 20:30:09,714 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.078035\n",
      "2025-03-16 20:30:09,715 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088559\n",
      "2025-03-16 20:30:09,913 - sciml.model.deeponet.deeponet - INFO - Epoch 292/300\n",
      "2025-03-16 20:30:09,914 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077354\n",
      "2025-03-16 20:30:09,915 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.085943\n",
      "Training progress:  97%|█████████▋| 292/300 [00:54<00:00,  9.46it/s]2025-03-16 20:30:10,119 - sciml.model.deeponet.deeponet - INFO - Epoch 293/300\n",
      "2025-03-16 20:30:10,120 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077808\n",
      "2025-03-16 20:30:10,120 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.086868\n",
      "2025-03-16 20:30:10,335 - sciml.model.deeponet.deeponet - INFO - Epoch 294/300\n",
      "2025-03-16 20:30:10,335 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.076710\n",
      "2025-03-16 20:30:10,336 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.088768\n",
      "Training progress:  98%|█████████▊| 294/300 [00:55<00:00,  8.12it/s]2025-03-16 20:30:10,560 - sciml.model.deeponet.deeponet - INFO - Epoch 295/300\n",
      "2025-03-16 20:30:10,561 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077680\n",
      "2025-03-16 20:30:10,561 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.086711\n",
      "2025-03-16 20:30:10,771 - sciml.model.deeponet.deeponet - INFO - Epoch 296/300\n",
      "2025-03-16 20:30:10,772 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077033\n",
      "2025-03-16 20:30:10,773 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087371\n",
      "Training progress:  99%|█████████▊| 296/300 [00:55<00:00,  7.07it/s]2025-03-16 20:30:11,001 - sciml.model.deeponet.deeponet - INFO - Epoch 297/300\n",
      "2025-03-16 20:30:11,002 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077075\n",
      "2025-03-16 20:30:11,003 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087330\n",
      "2025-03-16 20:30:11,226 - sciml.model.deeponet.deeponet - INFO - Epoch 298/300\n",
      "2025-03-16 20:30:11,228 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.076707\n",
      "2025-03-16 20:30:11,229 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.086787\n",
      "Training progress:  99%|█████████▉| 298/300 [00:55<00:00,  6.25it/s]2025-03-16 20:30:11,455 - sciml.model.deeponet.deeponet - INFO - Epoch 299/300\n",
      "2025-03-16 20:30:11,456 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.076972\n",
      "2025-03-16 20:30:11,459 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.087567\n",
      "Training progress: 100%|█████████▉| 299/300 [00:56<00:00,  5.90it/s]2025-03-16 20:30:11,683 - sciml.model.deeponet.deeponet - INFO - Epoch 300/300\n",
      "2025-03-16 20:30:11,684 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.077445\n",
      "2025-03-16 20:30:11,684 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.086755\n",
      "Training progress: 100%|██████████| 300/300 [00:56<00:00,  5.32it/s]\n"
     ]
    }
   ],
   "source": [
    "loss_history_train,loss_history_test = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30882325768470764, 0.29007488489151, 0.2792689800262451, 0.27800390124320984, 0.27523261308670044, 0.26799827814102173, 0.260322630405426, 0.2544909417629242, 0.24987861514091492, 0.24539566040039062, 0.24109068512916565, 0.23686739802360535, 0.23272818326950073, 0.22813370823860168, 0.22260989248752594, 0.21647436916828156, 0.2103065699338913, 0.204277902841568, 0.1979970932006836, 0.1912684291601181, 0.18466228246688843, 0.1783059686422348, 0.17161796987056732, 0.16524341702461243, 0.15980440378189087, 0.15443375706672668, 0.14962205290794373, 0.14602860808372498, 0.14275559782981873, 0.14045500755310059, 0.1387505680322647, 0.13744114339351654, 0.1369536966085434, 0.13612723350524902, 0.1355879306793213, 0.13447457551956177, 0.13335880637168884, 0.13188953697681427, 0.13046690821647644, 0.1289093792438507, 0.12756119668483734, 0.12622478604316711, 0.12509839236736298, 0.1240396574139595, 0.1232355386018753, 0.12231305241584778, 0.12147177755832672, 0.12050077319145203, 0.11952976882457733, 0.1184631735086441, 0.11751639097929001, 0.11657045781612396, 0.1157039999961853, 0.11478021740913391, 0.11388722062110901, 0.11290118843317032, 0.11195223033428192, 0.11104670912027359, 0.11023879051208496, 0.1095176413655281, 0.10878600180149078, 0.10801178216934204, 0.107208251953125, 0.10641877353191376, 0.10563778877258301, 0.1048978865146637, 0.10420148074626923, 0.10348966717720032, 0.10276099294424057, 0.10206101834774017, 0.10141108930110931, 0.10088015347719193, 0.10046617686748505, 0.10009448230266571, 0.0998949408531189, 0.09894934296607971, 0.09798453748226166, 0.09763757884502411, 0.09749153256416321, 0.09710471332073212, 0.09631489217281342, 0.09575116634368896, 0.09559899568557739, 0.09540928900241852, 0.09502286463975906, 0.09429246187210083, 0.09393782913684845, 0.09388050436973572, 0.09351959824562073, 0.09299860894680023, 0.09253670275211334, 0.09233924746513367, 0.09224487841129303, 0.09194909036159515, 0.09150958806276321, 0.09102396667003632, 0.0907963365316391, 0.09070715308189392, 0.0905604213476181, 0.09035149216651917, 0.08988197147846222, 0.08946427702903748, 0.08917168527841568, 0.08900336921215057, 0.08890093863010406, 0.08876050263643265, 0.08869314193725586, 0.0884956419467926, 0.08824197202920914, 0.08780808001756668, 0.08744626492261887, 0.08712917566299438, 0.08691294491291046, 0.08676400780677795, 0.08665262162685394, 0.08673784881830215, 0.08686201274394989, 0.08744125813245773, 0.08713840693235397, 0.08665937185287476, 0.0854651927947998, 0.08533696830272675, 0.08614793419837952, 0.08615417778491974, 0.0855780765414238, 0.0846073105931282, 0.0848599374294281, 0.08556012809276581, 0.0849439799785614, 0.08417584002017975, 0.08395367860794067, 0.08429818600416183, 0.08439408987760544, 0.0836770236492157, 0.08339853584766388, 0.08357969671487808, 0.0836271122097969, 0.08344585448503494, 0.08302786946296692, 0.08279584348201752, 0.08290667831897736, 0.08302827179431915, 0.08307204395532608, 0.08274553716182709, 0.08236954361200333, 0.08219538629055023, 0.0821647197008133, 0.08229580521583557, 0.08229047060012817, 0.08238351345062256, 0.08218763768672943, 0.08209146559238434, 0.08182039111852646, 0.0815906822681427, 0.08142158389091492, 0.08128548413515091, 0.08121655881404877, 0.08111371099948883, 0.08104224503040314, 0.08094935864210129, 0.0808672308921814, 0.08079174906015396, 0.0807030200958252, 0.08064137399196625, 0.08058632910251617, 0.08068309724330902, 0.08120585978031158, 0.08304868638515472, 0.08903476595878601, 0.08588870614767075, 0.08216562867164612, 0.08067852258682251, 0.08406136929988861, 0.08428739756345749, 0.08019466698169708, 0.08467987179756165, 0.08511850982904434, 0.08123403787612915, 0.08625175058841705, 0.08158666640520096, 0.08267886936664581, 0.08289304375648499, 0.08032011985778809, 0.08186926692724228, 0.08014199882745743, 0.08155716955661774, 0.08041591942310333, 0.08038410544395447, 0.0807030126452446, 0.07983501255512238, 0.08029751479625702, 0.07960684597492218, 0.08022866398096085, 0.07958883047103882, 0.07969005405902863, 0.07977709174156189, 0.07927881181240082, 0.07961730659008026, 0.07917255163192749, 0.07940554618835449, 0.07925765216350555, 0.07905924320220947, 0.07927466928958893, 0.0789400264620781, 0.07907001674175262, 0.07897455990314484, 0.07884357869625092, 0.07889746874570847, 0.07867137342691422, 0.07872271537780762, 0.07875899970531464, 0.07861997187137604, 0.07869986444711685, 0.07861603051424026, 0.07843794673681259, 0.07849403470754623, 0.07846614718437195, 0.07845768332481384, 0.07852931320667267, 0.07837189733982086, 0.07824409753084183, 0.078240305185318, 0.07822082936763763, 0.07825300097465515, 0.07837492227554321, 0.07816728949546814, 0.07796115428209305, 0.07800408452749252, 0.07819052040576935, 0.07811839133501053, 0.0778885930776596, 0.07783520221710205, 0.07785668969154358, 0.07790331542491913, 0.07786761224269867, 0.07778501510620117, 0.07767665386199951, 0.07760893553495407, 0.07758504152297974, 0.07765525579452515, 0.07774481177330017, 0.07772283256053925, 0.07753275334835052, 0.07741662114858627, 0.07737724483013153, 0.07737486064434052, 0.0773795023560524, 0.07735374569892883, 0.07733142375946045, 0.07737784832715988, 0.07738284766674042, 0.0774480402469635, 0.07721869647502899, 0.07718899846076965, 0.0773492231965065, 0.07744671404361725, 0.07740119099617004, 0.07716260850429535, 0.07715301960706711, 0.07753222435712814, 0.07708465307950974, 0.07697354257106781, 0.07707912474870682, 0.07709473371505737, 0.07716724276542664, 0.07721103727817535, 0.07740779966115952, 0.07816013693809509, 0.0784485787153244, 0.07939969748258591, 0.07822103798389435, 0.0775182694196701, 0.07703832536935806, 0.07687003910541534, 0.0774325504899025, 0.07791290432214737, 0.07866210490465164, 0.07763075083494186, 0.07667668163776398, 0.07696367055177689, 0.07786440849304199, 0.07846714556217194, 0.07789048552513123, 0.07773509621620178, 0.07695558667182922, 0.07931552827358246, 0.07940368354320526, 0.07826440036296844, 0.07775673270225525, 0.0769793912768364, 0.07803468406200409, 0.07735440880060196, 0.07780788838863373, 0.07671021670103073, 0.07768021523952484, 0.07703292369842529, 0.07707464694976807, 0.07670677453279495, 0.07697199285030365, 0.07744545489549637]\n"
     ]
    }
   ],
   "source": [
    "print(loss_history_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history_train,color='blue')\n",
    "plt.plot(loss_history_test,color='red')\n",
    "plt.legend(['train','test'])\n",
    "#log \n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
